#### 前言

Kubernetes作为当今最流行的容器编排平台之一，为企业提供了高效管理和部署容器化应用的解决方案。然而，构建一个稳定、可靠的Kubernetes集群对于确保应用程序的高可用性至关重要

在本文中，我们将探讨如何设计和部署一个多Master高可用集群架构，多Master节点架构旨在避免单点故障，并提高集群的可用性，确保业务持续运行

**实际生产环境架构设计**

- Master节点数量: 至少部署3个Master节点，以实现高可用性。这些Master节点将共同管理集群状态，并通过选举机制选择活跃的Leader节点

- node节点数量：至少3台Node节点可以提供一定程度的高可用性，因为即使一台Node节点出现故障，集群仍然可以继续运行。更多的Node节点可以提供更高级别的可用性和负载均衡

- 负载均衡器: 在Master节点前部署负载均衡器（如HAProxy、Nginx等），用于将流量均衡到不同的Master节点上，确保请求能够被正确路由到活跃的Leader节点

- Etcd集群: 部署独立的Etcd集群，用于存储Kubernetes集群的状态信息。Etcd是Kubernetes的数据存储后端，需要保证其高可用性和数据一致性


- 故障恢复和自动化: 配置故障恢复机制，包括自动故障转移、自动扩展和自动化部署，以减少人为干预并提高集群的稳定性

#### 集群规划信息

| 操作系统  | 主机名       | IP地址       | 规格 | 主要软件                                                     |
| --------- | ------------ | ------------ | ---- | ------------------------------------------------------------ |
| centos7.9 | master01     | 192.168.0.50 | 1C2G | apiserver、controller-manager、scheduler、etcd、HAProxy、KeepAlived |
| centos7.9 | master02     | 192.168.0.51 | 1C2G | apiserver、controller-manager、scheduler、etcd、HAProxy、KeepAlived |
| centos7.9 | master03     | 192.168.0.52 | 1C2G | apiserver、controller-manager、scheduler、etcd、HAProxy、KeepAlived |
| centos7.9 | node01       | 192.168.0.60 | 1C2G | kubelet、kube-proxy docker                                   |
| centos7.9 | node02       | 192.168.0.61 | 1C2G | kubelet、kube-proxy docker                                   |
| centos7.9 | node03       | 192.168.0.62 | 1C2G | kubelet、kube-proxy docker                                   |
| /         | apiserver-lb | 192.168.0.40 | /    | apiserver的负载均衡器IP地址                                  |

> [!CAUTION]
>
> **Pod 网段和 service 、宿主机网段不能有重复，以免出现冲突，导致不可用**



| 网段名称           | 网段           |
| ------------------ | -------------- |
| 主机节点网段       | 192.168.0.0/24 |
| Service网段        | 10.96.0.0/16   |
| Pod网段            | 172.16.0.0/16  |
| kubernetes service | 10.96.0.1      |
| Core DNS           | 10.96.0.10     |



![image-20241112211555572](./images/2.%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/image-20241112211555572-1732758436160-1-1736838949834-33.png)



## 一、部署步骤



### 1.基础环境

#### 1.1）配置主机名和DNS

```bash
#修改主机名
hostnamectl set-hostname master01
bash

hostnamectl set-hostname master02
bash

hostnamectl set-hostname master03
bash

hostnamectl set-hostname node01
bash

hostnamectl set-hostname node02
bash

hostnamectl set-hostname node03
bash

#配置DNS
cat >> /etc/hosts << EOF

192.168.0.40 master-lb
192.168.0.50 master01
192.168.0.51 master02
192.168.0.52 master03
192.168.0.60 node01
192.168.0.61 node02
192.168.0.62 node03
EOF
```

#### 1.2）配置免密并优化sshd

```
ssh-keygen -t rsa
for i in master01 master02 master03 node01 node02 node03; do ssh-copy-id -i .ssh/id_rsa.pub $i;done

# master01 配置完成后 将文件copy一份给到其他master机器
scp ~/.ssh/id_rsa root@master02:~/.ssh/
scp ~/.ssh/id_rsa root@master03:~/.ssh/

优化 sshd 服务配置
sed -i 's@#UseDNS yes@UseDNS no@g' /etc/ssh/sshd_config
sed -i 's@^GSSAPIAuthentication yes@GSSAPIAuthentication no@g' /etc/ssh/sshd_config
- UseDNS选项:
打开状态下，当客户端试图登录SSH服务器时，服务器端先根据客户端的IP地址进行DNS PTR反向查询出客户端的主机名，然后根据查询出的客户端主机名进行DNS正向A记录查询，验证与其原始IP地址是否一致，这是防止客户端欺骗的一种措施，但一般我们的是动态IP不会有PTR记录，打开这个选项不过是在白白浪费时间而已，不如将其关闭。
- GSSAPIAuthentication:
当这个参数开启（ GSSAPIAuthentication  yes ）的时候，通过SSH登陆服务器时候会有些会很慢！这是由于服务器端启用了GSSAPI。登陆的时候客户端需要对服务器端的IP地址进行反解析，如果服务器的IP地址没有配置PTR记录，那么就容易在这里卡住了。
```

#### 1.3）配置YUM 安装基础软件

```bash
curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo
curl  -s -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo

yum -y install bind-utils expect rsync wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git ntpdate bash-completion
```

#### 1.4）关闭 SWAP、Selinux、Firewalld、dnsmasq

```bash
swapoff -a && sed -ri 's/.*swap.*/#&/' /etc/fstab

将包含 "swap" 字串的行首添加 '#' 进行注释
r 代表使用扩展正则表达式，而 -i 使命令对文件进行原地编辑，即直接修改文件而不是输出到标准输出。
.*swap.*：这个模式匹配任何包含 "swap" 字串的行。.* 表示任何字符（除了换行符）出现任意次数。
#&：这表示把匹配到的整行（&代表被匹配的整个模式）前加上一个井号 #

sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config

systemctl disable --now firewalld
systemctl disable --now dnsmasq

# centos7 需要关比 NM，centos8不需要
systemctl disable --now NetworkManager
```

#### 1.5）配置时间同步

```bash
ln -svf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
ntpdate ntp.aliyun.com

        - 定期任务同步(也可以使用"crontab -e"手动编辑，但我更推荐我下面的做法，可以非交互)
echo "*/5 * * * * /usr/sbin/ntpdate ntp.aliyun.com" > /var/spool/cron/root
crontab -l
```

#### 1.6）内核参数调优

```bash
if ! grep HISTTIMEFORMAT /etc/bashrc; then
    echo 'export HISTTIMEFORMAT="%F %T `whoami` "' >> /etc/bashrc
fi

cat >> /etc/security/limits.conf <<'EOF'
* soft nofile 655360
* hard nofile 131072
* soft nproc 655350
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
EOF

cat > /etc/sysctl.d/k8s.conf <<EOF
net.ipv4.ip_forward = 1
net.ipv4.tcp_syncookies = 1
vm.panic_on_oom=0
net.ipv4.tcp_timestamps = 0
net.ipv4.tcp_tw_reuse = 1
kernel.pid_max = 4194303
fs.file-max = 52706963
net.core.netdev_max_backlog = 50000
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.core.rmem_default = 16777216
net.core.wmem_default = 16777216
net.core.somaxconn = 32768
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
kernel.threads-max = 798720
kernel.stack_tracer_enabled = 1
EOF
sysctl --system


-----------------------------------------------------------------
    # 启用 IP 转发，对于 Kubernetes 节点是必需的，因为它需要转发流量
    net.ipv4.ip_forward = 1
    # 启用 SYN cookies，帮助抵御 SYN 洪水攻击
    net.ipv4.tcp_syncookies = 1
    # 禁止在内存耗尽时触发内核 panic，让系统尝试处理内存不足
    vm.panic_on_oom=0
    # 禁用 TCP 时间戳可以减少一些网络攻击的面
    net.ipv4.tcp_timestamps = 0
    # 允许重用处于 TIME-WAIT 状态的 TCP sockets，适用于高性能的服务器以减少等待时间
    net.ipv4.tcp_tw_reuse = 1
    # 调整为允许更多的 PIDs，因为容器化环境可能会产生大量进程
    kernel.pid_max = 4194303
    # 提高文件系统的文件句柄限制，防止在大量文件操作时出现耗尽文件描述符的情况
    fs.file-max = 52706963
    # 提高网络设备接收队列长度，有助于应对高速网络接口下的数据包
    net.core.netdev_max_backlog = 50000
    # 调整网络缓冲区大小，提高数据传输性能
    net.core.rmem_max = 16777216
    net.core.wmem_max = 16777216
    net.core.rmem_default = 16777216
    net.core.wmem_default = 16777216
    # 设置容器网络接口的流量控制参数
    net.core.somaxconn = 32768
    # 调整 TCP 栈的内存分配，以适应更大的并发连接数
    net.ipv4.tcp_rmem = 4096 87380 16777216
    net.ipv4.tcp_wmem = 4096 65536 16777216
    # 提高系统中允许的跟踪进程数量，用于调试和监控
    kernel.threads-max = 798720
    kernel.stack_tracer_enabled = 1
```

#### 1.7）修改终端颜色

```bash
cat <<EOF >>  ~/.bashrc 
PS1='[\[\e[34;1m\]\u@\[\e[0m\]\[\e[32;1m\]\H\[\e[0m\]\[\e[31;1m\] \W\[\e[0m\]]# '
EOF
source ~/.bashrc
```

#### 1.8）升级内核

```bash
#1.master01节点下载并安装内核软件包
wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm
wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm

#2.master01节点将下载的软件包同步到其他节点
for i in master02 master03 node01 node02 node03; do scp kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm root@${i}:/root/

#3.所有节点执行安装升级Linux内核命令
yum -y localinstall kernel-ml*

#4.更改内核启动顺序
grub2-set-default  0 && grub2-mkconfig -o /etc/grub2.cfg
grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"
grubby --default-kernel

#5.所有节点更新软件版本，但不需要更新内核，因为内核已经更新到了指定的版本
yum -y update --exclude=kernel*

```

#### 1.9）点安装ipvsadm以实现kube-proxy的负载均衡

```bash
yum install -y ipvsadm ipset sysstat conntrack libseccomp 

cat > /etc/modules-load.d/ipvs.conf << 'EOF'
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF
```

#### 1.10）验证内核和模块

```bash
#验证加载的模块
lsmod | grep --color=auto -e ip_vs -e nf_conntrack

#验证内核是否升级成功
uname -r

4.19.12-1.el7.elrepo.x86_64
```



### 2.安装containerd

#### 2.1）配置 yum源 安装containd

```bash
curl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

yum install -y containerd.io
```

#### 2.2）配置 containerd 需要的模块

```bash
#1.临时手动加载模块
modprobe -- overlay
modprobe -- br_netfilter

#开机自动加载
cat > /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

##overlay 是一种文件系统，允许将一个文件系统（称为上层）覆盖在另一个文件系统（称为下层）之上。它常用于容器技术，如 Docker 和 containerd，因为它支持高效的层叠存储，便于创建和管理容器镜像。
##br_netfilter 是一个内核模块，用于允许 Linux 的桥接网络流量进行网络过滤。它允许iptables等防火墙工具对通过 Linux 桥接的网络流量进行处理。
```

#### 2.3）修改containerd配置

```bash
#1.重新初始化containerd的配置文件
containerd config default | tee /etc/containerd/config.toml 

#2.修改Cgroup的管理者为systemd组件
sed -ri 's#(SystemdCgroup = )false#\1true#' /etc/containerd/config.toml 
grep SystemdCgroup /etc/containerd/config.toml

#3.修改pause的基础镜像名称
sed -i 's#registry.k8s.io/pause:3.6#registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.7#' /etc/containerd/config.toml
grep sandbox_image /etc/containerd/config.toml

#4.配置crictl客户端连接的运行时位置
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
```

#### 2.4）启动containerd

```yaml
systemctl daemon-reload
systemctl enable --now containerd
systemctl status containerd
```



### 3.安装etcd数据库

> [!NOTE]  
>
> 请注意，以下操作并不是所有节点都需要操作

#### 3.1）下载etcd数据库的二进制安装包

```bash
wget https://github.com/etcd-io/etcd/releases/download/v3.5.10/etcd-v3.5.10-linux-amd64.tar.gz
```

#### 3.2）解压etcd的二进制程序包到PATH环境变量路径

```bash
#解压到/usr/local/bin
tar -xf etcd-v3.5.10-linux-amd64.tar.gz --strip-components=1 -C /usr/local/bin etcd-v3.5.10-linux-amd64/etcd{,ctl}

#查看版本
etcdctl version

#复制到其他两个master节点上
for i in master02 master03; do echo $i; scp /usr/local/bin/etcd* $i:/usr/local/bin/; done
```



### 4.安装K8S组件

#### 4.1）下载k8s的二进制软件包

```bash
#使用命令下载
wget https://cdn.dl.k8s.io/release/v1.28.15/kubernetes-server-linux-amd64.tar.gz
```

**使用 GitHub 仓库下载  `https://github.com/kubernetes/kubernetes/tree/release-1.28`**

![image-20241101025317508](./images/2.%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/image-20241101025317508-1732758436160-2-1736838949834-32.png)

![image-20241101025434999](./images/2.%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/image-20241101025434999-1732758436160-3-1736838949834-31.png)

![image-20241101025852074](./images/2.%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/image-20241101025852074-1732758436160-4-1736838949834-35.png)

#### 4.2）解压K8S的二进制程序包到PATH环境变量路径

```bash
#解压
tar -xf kubernetes-server-linux-amd64.tar.gz  --strip-components=3 -C /usr/local/bin kubernetes/server/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy}
 
#查看kubelet的版本 
kubelet --version
```

#### 4.3）将软件包下发到对于节点

```bash
for i in master02 master03; do echo $i; scp /usr/local/bin/kube{let,ctl,-apiserver,-controller-manager,-scheduler,-proxy} $i:/usr/local/bin/; done
for i in node{01,02,03}; do echo $i; scp /usr/local/bin/kube{let,-proxy} $i:/usr/local/bin/; done
```



### 5.生成etcd和k8s证书文件

#### 5.1）安装cfssl证书管理工具

##### 5.1.1 cfssl下载

**GitHub 下载地址:    `https://github.com/cloudflare/cfssl`**

```bash
curl -L -o /usr/local/bin/cfssl https://github.com/cloudflare/cfssl/releases/download/1.6.4/cfssl_1.6.4_linux_amd64
curl -L -o /usr/local/bin/cfssljson https://github.com/cloudflare/cfssl/releases/download/1.6.4/cfssljson_1.6.4_linux_amd64
curl -L -o /usr/local/bin/cfssl-certinfo https://github.com/cloudflare/cfssl/releases/download/1.6.4/cfssl-certinfo_1.6.4_linux_amd64
```

##### 5.1.2 将cfssl二进制程序包到PATH环境变量路径

```bash
#给二进制执行文件添加执行权限
chmod +x cfssl*
#将本目录的文件名统一减去 _1.6.4_linux_amd64
rename _1.6.4_linux_amd64 "" *
#移动目录
mv cfssl* /usr/local/bin/

[root@master01 ~]# ll /usr/local/bin/cfssl*
-rwxr-xr-x 1 root root 12054528 Nov  1 02:35 /usr/local/bin/cfssl
-rwxr-xr-x 1 root root  9560064 Nov  1 02:36 /usr/local/bin/cfssl-certinfo
-rwxr-xr-x 1 root root  7643136 Nov  1 02:36 /usr/local/bin/cfssljson
```

#### 5.2）生成etcd证书

##### 5.2.1 创建证书存放目录

```bash
mkdir -p /etc/etcd/{ssl,pki} /etc/kubernetes/{ssl,pki}
```

##### 5.2.2 创建 etcd CA 根证书

```yaml
cd /etc/etcd/pki

#创建证书配置文件
cat > etcd-ca-csr.json  << EOF
{
  "CN": "etcd",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "etcd",
      "OU": "Etcd Security"
    }
  ],
  "ca": {
    "expiry": "876000h"
  }
}
EOF

#CN (Common Name)："etcd" 表示这个证书的通用名称，通常是服务的名称。
#key：定义密钥的参数
#algo：密钥算法，这里使用的是 rsa。
#size：密钥的大小，这里是 2048 位，表示密钥的安全性。
#names：包含证书的详细信息

#C：国家（Country），这里是 "CN"（中国）。
#ST：省（State），这里是 "Beijing"。
#L：市（Locality），这里也是 "Beijing"。
#O：组织（Organization），这里是 "etcd"。
#OU：组织单位（Organizational Unit），这里是 "Etcd Security"。
#ca：CA 的相关配置

#expiry：有效期，这里设为 "876000h"，相当于 100 年。

#生成 CA 根证书
cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare /etc/etcd/ssl/etcd-ca

[root@master01 pki]# ll /etc/etcd/ssl
total 12
-rw-r--r-- 1 root root 1050 Nov 18 10:35 etcd-ca.csr
-rw------- 1 root root 1675 Nov 18 10:35 etcd-ca-key.pem
-rw-r--r-- 1 root root 1318 Nov 18 10:35 etcd-ca.pem
#etcd-ca.csr（证书签名请求）
#etcd-ca-key.pem（CA 私钥）
#etcd-ca.pem（CA 证书）
```

##### 5.2.3 创建证书配置文件和CSR文件

```bash
cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "876000h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ],
        "expiry": "876000h"
      }
    }
  }
}
EOF


#生成证书的CSR文件： 证书签发请求文件，配置了一些域名，公司，单位 
cat > etcd-csr.json << EOF
{
  "CN": "etcd",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "etcd",
      "OU": "Etcd Security"
    }
  ]
}
EOF
```

##### 5.2.4 基于自建 CA 证书生成 etcd 证书

```bash
[root@k8s-master01 pki]# cfssl gencert \
  -ca=/etc/etcd/ssl/etcd-ca.pem \
  -ca-key=/etc/etcd/ssl/etcd-ca-key.pem \
  -config=ca-config.json \
  --hostname=127.0.0.1,master01,master02,master03,192.168.0.50,192.168.0.51,192.168.0.52 \
  --profile=kubernetes \
  etcd-csr.json  | cfssljson -bare /etc/etcd/ssl/etcd
  
#cfssl gencert：生成证书的命令，使用自建 CA 证书和私钥。
#-ca 和 -ca-key：指定之前生成的 CA 证书和私钥。
#-config=ca-config.json：使用前面创建的 CA 配置文件。
#--hostname：指定 所有的 etcd 服务的主机名和 IP 地址，这些会包含在证书中。
#--profile=kubernetes：指定使用 Kubernetes 的配置文件。
#etcd-csr.json：输入的 CSR 文件，包含证书请求的参数。
# | cfssljson -bare /etc/etcd/ssl/etcd：输出生成的证书和密钥，文件名为 etcd，包括：
#etcd.csr
#etcd.pem
#etcd.pem
 
 
[root@master01 pki]# ll /etc/etcd/ssl/
total 24
-rw-r--r-- 1 root root 1050 Nov 18 10:35 etcd-ca.csr
-rw------- 1 root root 1675 Nov 18 10:35 etcd-ca-key.pem
-rw-r--r-- 1 root root 1318 Nov 18 10:35 etcd-ca.pem
-rw-r--r-- 1 root root 1115 Nov 18 11:17 etcd.csr
-rw------- 1 root root 1679 Nov 18 11:17 etcd-key.pem
-rw-r--r-- 1 root root 1448 Nov 18 11:17 etcd.pem
```

##### 5.2.5 将etcd 证书拷贝到其他master节点

```bash
for i in master0{2,3};do echo $i ;ssh $i "mkdir -pv /etc/etcd/ssl";scp /etc/etcd/ssl/*pem $i:/etc/etcd/ssl ;done

[root@master01 pki]# for i in master0{2,3};do echo $i ;ssh $i "mkdir -pv /etc/etcd/ssl";scp /etc/etcd/ssl/*pem $i:/etc/etcd/ssl ;done
master02
mkdir: created directory ‘/etc/etcd’
mkdir: created directory ‘/etc/etcd/ssl’
etcd-ca-key.pem                                                                                            100% 1675     1.4MB/s   00:00
etcd-ca.pem                                                                                                100% 1318   431.8KB/s   00:00
etcd-key.pem                                                                                               100% 1679   964.2KB/s   00:00
etcd.pem                                                                                                   100% 1448     1.2MB/s   00:00
master03
mkdir: created directory ‘/etc/etcd’
mkdir: created directory ‘/etc/etcd/ssl’
etcd-ca-key.pem                                                                                            100% 1675   761.0KB/s   00:00
etcd-ca.pem                                                                                                100% 1318   891.8KB/s   00:00
etcd-key.pem                                                                                               100% 1679     1.8MB/s   00:00
etcd.pem                                                                                                   100% 1448     1.7MB/s   00:00
```

#### 5.3）生成k8s组件相关证书

##### 5.3.1 创建kubernetes  ca证书

```bash
cd /etc/kubernetes/pki
#创建 k8s 根证书配置文件
cat > k8s-ca-csr.json << EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "Kubernetes",
      "OU": "Kubernetes-manual"
    }
  ],
  "ca": {
    "expiry": "876000h"
  }
}
EOF

#创建根证书
cfssl gencert -initca k8s-ca-csr.json | cfssljson -bare /etc/kubernetes/ssl/k8s-ca
#查看
[root@master01 pki]# ll /etc/kubernetes/ssl/
total 12
-rw-r--r-- 1 root root 1070 Nov 18 11:31 k8s-ca.csr（证书签名请求）
-rw------- 1 root root 1679 Nov 18 11:31 k8s-ca-key.pem（CA 私钥）
-rw-r--r-- 1 root root 1363 Nov 18 11:31 k8s-ca.pem（CA 证书） 
```

##### 5.3.2 创建证书配置文件和CSR文件

```bash
cat > k8s-ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "876000h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ],
        "expiry": "876000h"
      }
    }
  }
}
EOF


cat > apiserver-csr.json << EOF 
{
  "CN": "kube-apiserver",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "Kubernetes",
      "OU": "Kubernetes-manual"
    }
  ]
}
EOF
```

##### 5.3.3 基于自建k8s ca证书生成apiServer的证书

```bash
cfssl gencert \
  -ca=/etc/kubernetes/ssl/k8s-ca.pem \
  -ca-key=/etc/kubernetes/ssl/k8s-ca-key.pem \
  -config=k8s-ca-config.json \
  --hostname=10.96.0.1,192.168.0.40,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.0.50,192.168.0.51,192.168.0.52 \
  --profile=kubernetes \
  apiserver-csr.json | cfssljson -bare /etc/kubernetes/ssl/apiserver

  
# cfssl gencert：生成证书的命令，使用自建 CA 证书和私钥。
#  -ca=/etc/kubernetes/ssl/k8s-ca.pem \  -ca：指定 CA 根证书的路径。
#   -ca-key=/etc/kubernetes/ssl/k8s-ca-key.pem \  -ca-key：指定 CA 根证书对应的私钥路径。
#   -config=k8s-ca-config.json \  -config：指定证书生成的配置文件，定义证书属性。
#   --hostname=10.96.0.1,192.168.0.40,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,192.168.0.50,192.168.0.51,192.168.0.52 \ 
	# 10.96.0.1 是 Kubernetes 服务的默认 IP 地址，SVC 网段的第一个地址
	# 192.168.0.40 是集群的虚拟 IP（VIP）地址，用于 Kubernetes API 服务的高可用性。
	# 127.0.0.1 表示本地地址，可以用于本机访问。
	# kubernetes.default 这是位于默认命名空间下的Kubernetes API服务的服务名。
	# kubernetes.default.svc 这是服务名，指定了在同一命名空间（默认命名空间）下的服务访问。
	# kubernetes.default.svc.cluster 这是一个更具体的服务名，指定了在Kubernetes集群内的默认命名空间下的服务访问。
	# kubernetes.default.svc.cluster.local 是Kubernetes集群的完全限定域名(FQDN)用于访问默认命名空间下的服务。
	# 192.168.0.50, 192.168.0.51, 192.168.0.52  是集群中各个 master 节点的 IP 地址。
# --profile=kubernetes \   --profile：指定要使用的证书配置文件。
# apiserver-csr.json \   apiserver-csr.json：包含证书签名请求详细信息的 JSON 文件。
# | cfssljson -bare /etc/kubernetes/ssl/apiserver  ：将生成的证书和密钥格式化为文件，并保存到指定路径。

#验证
[root@master01 pki]# ll /etc/kubernetes/ssl/apiserver*
-rw-r--r-- 1 root root 1297 Nov 18 14:37 /etc/kubernetes/ssl/apiserver.csr
-rw------- 1 root root 1675 Nov 18 14:37 /etc/kubernetes/ssl/apiserver-key.pem
-rw-r--r-- 1 root root 1692 Nov 18 14:37 /etc/kubernetes/ssl/apiserver.pem
```

#### 5.4）生成第三方组件与apiServer通信的聚合证书

聚合证书的作用就是让第三方组件(比如metrics-server等)能够拿这个证书文件和apiServer进行通信。

##### 5.4.1 生成聚合证书的用于自建ca的CSR文件

```bash
cat > front-proxy-ca-csr.json << EOF
{
  "CN": "kubernetes",
  "key": {
     "algo": "rsa",
     "size": 2048
  }
}
EOF
```

##### 5.4.2 生成聚合证书的自建ca证书

```bash
cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare /etc/kubernetes/ssl/front-proxy-ca

[root@master01 pki]# ll /etc/kubernetes/ssl/front-proxy-ca*
-rw-r--r-- 1 root root  891 Nov 18 14:37 /etc/kubernetes/ssl/front-proxy-ca.csr
-rw------- 1 root root 1675 Nov 18 14:37 /etc/kubernetes/ssl/front-proxy-ca-key.pem
-rw-r--r-- 1 root root 1094 Nov 18 14:37 /etc/kubernetes/ssl/front-proxy-ca.pem
```

##### 5.4.3 生成聚合证书的用于客户端的CSR文件

```bash
cat > front-proxy-client-csr.json << EOF
{
  "CN": "front-proxy-client",
  "key": {
     "algo": "rsa",
     "size": 2048
  }
}
EOF
```

##### 5.4.4 基于聚合证书的自建ca证书签发聚合证书的客户端证书

```bash
cfssl gencert \
  -ca=/etc/kubernetes/ssl/front-proxy-ca.pem \
  -ca-key=/etc/kubernetes/ssl/front-proxy-ca-key.pem \
  -config=k8s-ca-config.json \
  -profile=kubernetes \
  front-proxy-client-csr.json | cfssljson -bare /etc/kubernetes/ssl/front-proxy-client

[root@master01 pki]# ll /etc/kubernetes/ssl/front-proxy-client*
-rw-r--r-- 1 root root  903 Nov 18 14:40 /etc/kubernetes/ssl/front-proxy-client.csr
-rw------- 1 root root 1675 Nov 18 14:40 /etc/kubernetes/ssl/front-proxy-client-key.pem
-rw-r--r-- 1 root root 1188 Nov 18 14:40 /etc/kubernetes/ssl/front-proxy-client.pem
```

#### 5.5）生成controller-manager证书及kubeconfig文件

##### 5.5.1 生成controller-manager的CSR文件

```bash
#创建一个证书签名请求（CSR）文件，包含控制器管理器的相关信息
cat > controller-manager-csr.json << EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes-manual"
    }
  ]
}
EOF


# CN（Common Name）：表示此证书的主体，使用 system:kube-controller-manager。
# key：指定密钥算法为 RSA，长度为 2048 位。
# names：提供有关证书的附加信息，包括国家（C）、省（ST）、市（L）、组织（O）和组织单位（OU）。
```

##### 5.5.2 基于自建k8s ca证书生成controller-manager的证书

```bash
#使用 CFSSL 工具生成控制器管理器的证书文件
cfssl gencert \
  -ca=/etc/kubernetes/ssl/k8s-ca.pem \
  -ca-key=/etc/kubernetes/ssl/k8s-ca-key.pem \
  -config=k8s-ca-config.json \
  -profile=kubernetes \
  controller-manager-csr.json | cfssljson -bare /etc/kubernetes/ssl/controller-manager
   

# -ca：指定 CA 根证书的路径。
# -ca-key：指定 CA 根证书的私钥路径。
# -config：证书配置文件，定义生成的证书的属性。
# -profile：指定证书生成的配置文件。
# controller-manager-csr.json：之前生成的 CSR 文件。
# 通过管道输出生成的证书和密钥文件到指定目录。

[root@master01 pki]# ll /etc/kubernetes/ssl/controller-manager* 
-rw-r--r-- 1 root root 1082 Nov 18 14:42 /etc/kubernetes/ssl/controller-manager.csr
-rw------- 1 root root 1679 Nov 18 14:42 /etc/kubernetes/ssl/controller-manager-key.pem
-rw-r--r-- 1 root root 1501 Nov 18 14:42 /etc/kubernetes/ssl/controller-manager.pem
```

##### 5.5.3 设置一个集群

```bash
#配置集群信息
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/k8s-ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.40:8443 \
  --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig
  
# --certificate-authority：指向 CA 证书，确保客户端可以验证 API Server 的身份。
# --embed-certs=true：将 CA 证书嵌入到 kubeconfig 文件中。
# --server：指定 Kubernetes API Server 的地址,这里我写的是VIP地址。
# --kubeconfig：指定 kubeconfig 文件的保存路径。
```

##### 5.5.4 设置一个用户项

```bash
#配置用户凭证 
kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=/etc/kubernetes/ssl/controller-manager.pem \
  --client-key=/etc/kubernetes/ssl/controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig
  
# --client-certificate：指定用户的客户端证书。
# --client-key：指定用户的客户端私钥。
# --embed-certs=true：将用户证书嵌入到 kubeconfig 文件中。
# --kubeconfig：指定 kubeconfig 文件的保存路径。  
```

##### 5.5.5 设置一个上下文环境

```bash
#创建一个上下文，将集群与用户关联起来，以便后续命令中使用
kubectl config set-context system:kube-controller-manager@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig
  
# --cluster：指定使用的集群名称。
# --user：指定与该上下文关联的用户。
```

##### 5.5.6 设置默认的上下文

```bash
kubectl config use-context system:kube-controller-manager@kubernetes \
  --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig
```

#### 5.6）生成scheduler证书及kubeconfig文件

##### 5.6.1 生成scheduler的CSR文件

```bash
cat > scheduler-csr.json << EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes-manual"
    }
  ]
}
EOF
```

##### 5.6.2 基于自建k8s ca证书生成 scheduler 的证书

```bash
cfssl gencert \
  -ca=/etc/kubernetes/ssl/k8s-ca.pem \
  -ca-key=/etc/kubernetes/ssl/k8s-ca-key.pem \
  -config=k8s-ca-config.json \
  -profile=kubernetes \
  scheduler-csr.json | cfssljson -bare /etc/kubernetes/ssl/scheduler
  
  
[root@master01 pki]# ll /etc/kubernetes/ssl/scheduler* 
-rw-r--r-- 1 root root 1058 Nov 18 15:07 /etc/kubernetes/ssl/scheduler.csr
-rw------- 1 root root 1675 Nov 18 15:07 /etc/kubernetes/ssl/scheduler-key.pem
-rw-r--r-- 1 root root 1476 Nov 18 15:07 /etc/kubernetes/ssl/scheduler.pem
```

##### 5.6.3 设置一个集群

```bash
#配置集群信息
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/k8s-ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.40:8443 \
  --kubeconfig=/etc/kubernetes/scheduler.kubeconfig
```

##### 5.6.4 设置一个用户项

```bash
#配置用户凭证 
kubectl config set-credentials system:kube-scheduler \
  --client-certificate=/etc/kubernetes/ssl/scheduler.pem \
  --client-key=/etc/kubernetes/ssl/scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=/etc/kubernetes/scheduler.kubeconfig
```

##### 5.6.5 设置一个上下文环境

```bash
#创建一个上下文，将集群与用户关联起来，以便后续命令中使用
kubectl config set-context system:kube-scheduler@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=/etc/kubernetes/scheduler.kubeconfig
```

##### 5.6.6 设置默认的上下文

```bash
kubectl config use-context system:kube-scheduler@kubernetes \
  --kubeconfig=/etc/kubernetes/scheduler.kubeconfig
```

#### 5.7）配置k8s集群管理员证书及kubeconfig文件

##### 5.6.1 生成管理员的CSR文件

```bash
cat > admin-csr.json << EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:masters",
      "OU": "Kubernetes-manual"
    }
  ]
}
EOF
```

##### 5.6.2 基于自建k8s ca证书生成管理员的证书

```bash
cfssl gencert \
  -ca=/etc/kubernetes/ssl/k8s-ca.pem \
  -ca-key=/etc/kubernetes/ssl/k8s-ca-key.pem \
  -config=k8s-ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare /etc/kubernetes/ssl/admin
  
  
[root@master01 pki]# ll /etc/kubernetes/ssl/admin* 
-rw-r--r-- 1 root root 1025 Nov 18 15:31 /etc/kubernetes/ssl/admin.csr
-rw------- 1 root root 1679 Nov 18 15:31 /etc/kubernetes/ssl/admin-key.pem
-rw-r--r-- 1 root root 1444 Nov 18 15:31 /etc/kubernetes/ssl/admin.pem
```

##### 5.6.3 设置一个集群

```bash
#配置集群信息
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/k8s-ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.40:8443 \
  --kubeconfig=/etc/kubernetes/admin.kubeconfig
```

##### 5.6.4 设置一个用户项

```bash
#配置用户凭证 
kubectl config set-credentials kube-admin \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --client-key=/etc/kubernetes/ssl/admin-key.pem \
  --embed-certs=true \
  --kubeconfig=/etc/kubernetes/admin.kubeconfig
```

##### 5.6.5 设置一个上下文环境

```bash
#创建一个上下文，将集群与用户关联起来，以便后续命令中使用
kubectl config set-context kube-admin@kubernetes \
  --cluster=kubernetes \
  --user=kube-admin \
  --kubeconfig=/etc/kubernetes/admin.kubeconfig
```

##### 5.6.6 设置默认的上下文

```bash
kubectl config use-context kube-admin@kubernetes \
  --kubeconfig=/etc/kubernetes/admin.kubeconfig
```

#### 5.8）创建ServiceAccount

##### 5.8.1 生成私钥

```bash
#此命令生成一个 2048 位的 RSA 私钥，并将其保存到指定的文件中。这个私钥可以用于服务账户的身份验证。
openssl genrsa -out /etc/kubernetes/ssl/sa.key 2048

# openssl: OpenSSL 是一个用于处理加密相关操作的命令行工具
# genrsa: 表示生成 RSA 密钥
# -out /etc/kubernetes/ssl/sa.keyy: 指定生成的私钥文件的保存路径，这里是 /etc/kubernetes/ssl/sa.key
# 2048: 指定密钥的位数，2048 位是当前推荐的安全密钥长度
```

##### 5.8.2 基于私钥创建公钥

```bash
#命令从私钥中提取出公钥，并将其保存到指定的文件中。公钥可以安全地分发给需要与之进行加密通信的实体，而私钥则应保持安全，不被外界访问
openssl rsa -in /etc/kubernetes/ssl/sa.key -pubout -out /etc/kubernetes/ssl/sa.pub

# openssl rsa: 这是 OpenSSL 的一个子命令，用于处理 RSA 密钥
# -in /etc/kubernetes/ssl/sa.key: 指定输入文件，即之前生成的私钥文件
# -pubout: 表示输出公钥，而不是私钥
# -out /etc/kubernetes/ssl/sa.pub: 指定生成的公钥文件的保存路径，这里是 /etc/kubernetes/ssl/sa.pub

[root@master01 pki]# ll /etc/kubernetes/ssl/sa*
-rw-r--r-- 1 root root 1675 Nov 18 15:53 /etc/kubernetes/ssl/sa.key
-rw-r--r-- 1 root root  451 Nov 18 15:54 /etc/kubernetes/ssl/sa.pub
```

#### 5.9）将master01节点K8S组件证书拷贝到其他master节点

```bash
for i in master0{2,3}; do echo $i;ssh $i "mkdir -pv /etc/kubernetes/ssl";scp /etc/kubernetes/ssl/* $i:/etc/kubernetes/ssl/ ;scp /etc/kubernetes/*kubeconfig $i:/etc/kubernetes/ ;done


[root@master01 pki]# for i in master0{2,3}; do echo $i;ssh $i "mkdir -pv /etc/kubernetes/ssl";scp /etc/kubernetes/ssl/* $i:/etc/kubernetes/ssl/ ;scp /etc/kubernetes/*kubeconfig $i:/etc/kubernetes/ ;done
master02
mkdir: created directory ‘/etc/kubernetes’
mkdir: created directory ‘/etc/kubernetes/ssl’
admin.csr                                                                                                  100% 1025   868.2KB/s   00:00    
admin-key.pem                                                                                              100% 1679     1.9MB/s   00:00    
admin.pem                                                                                                  100% 1444     1.2MB/s   00:00    
apiserver.csr                                                                                              100% 1297     1.7MB/s   00:00    
apiserver-key.pem                                                                                          100% 1675     1.3MB/s   00:00    
apiserver.pem                                                                                              100% 1692   347.0KB/s   00:00    
controller-manager.csr                                                                                     100% 1082   663.4KB/s   00:00    
controller-manager-key.pem                                                                                 100% 1679     1.6MB/s   00:00    
controller-manager.pem                                                                                     100% 1501   533.5KB/s   00:00    
front-proxy-ca.csr                                                                                         100%  891   631.0KB/s   00:00    
front-proxy-ca-key.pem                                                                                     100% 1675     1.2MB/s   00:00    
front-proxy-ca.pem                                                                                         100% 1094   752.6KB/s   00:00    
front-proxy-client.csr                                                                                     100%  903     1.1MB/s   00:00    
front-proxy-client-key.pem                                                                                 100% 1675     1.0MB/s   00:00    
front-proxy-client.pem                                                                                     100% 1188     1.2MB/s   00:00    
k8s-ca.csr                                                                                                 100% 1070   993.8KB/s   00:00    
k8s-ca-key.pem                                                                                             100% 1679     1.4MB/s   00:00    
k8s-ca.pem                                                                                                 100% 1363     1.3MB/s   00:00    
sa.key                                                                                                     100% 1675     1.8MB/s   00:00    
sa.pub                                                                                                     100%  451   493.2KB/s   00:00    
scheduler.csr                                                                                              100% 1058     1.5MB/s   00:00    
scheduler-key.pem                                                                                          100% 1675   676.9KB/s   00:00    
scheduler.pem                                                                                              100% 1476     1.2MB/s   00:00    
admin.kubeconfig                                                                                           100% 6364     3.4MB/s   00:00    
controller-manager.kubeconfig                                                                              100% 6520     3.0MB/s   00:00    
scheduler.kubeconfig                                                                                       100% 6444     6.5MB/s   00:00    
master03
mkdir: created directory ‘/etc/kubernetes’
mkdir: created directory ‘/etc/kubernetes/ssl’
admin.csr                                                                                                  100% 1025   343.5KB/s   00:00    
admin-key.pem                                                                                              100% 1679   963.0KB/s   00:00    
admin.pem                                                                                                  100% 1444     1.0MB/s   00:00    
apiserver.csr                                                                                              100% 1297     1.2MB/s   00:00    
apiserver-key.pem                                                                                          100% 1675     1.4MB/s   00:00    
apiserver.pem                                                                                              100% 1692     2.0MB/s   00:00    
controller-manager.csr                                                                                     100% 1082   841.7KB/s   00:00    
controller-manager-key.pem                                                                                 100% 1679   605.6KB/s   00:00    
controller-manager.pem                                                                                     100% 1501     1.6MB/s   00:00    
front-proxy-ca.csr                                                                                         100%  891   634.5KB/s   00:00    
front-proxy-ca-key.pem                                                                                     100% 1675     1.2MB/s   00:00    
front-proxy-ca.pem                                                                                         100% 1094     1.1MB/s   00:00    
front-proxy-client.csr                                                                                     100%  903     1.0MB/s   00:00    
front-proxy-client-key.pem                                                                                 100% 1675     1.5MB/s   00:00    
front-proxy-client.pem                                                                                     100% 1188     1.2MB/s   00:00    
k8s-ca.csr                                                                                                 100% 1070   975.7KB/s   00:00    
k8s-ca-key.pem                                                                                             100% 1679   584.4KB/s   00:00    
k8s-ca.pem                                                                                                 100% 1363     1.2MB/s   00:00    
sa.key                                                                                                     100% 1675     1.3MB/s   00:00    
sa.pub                                                                                                     100%  451   431.9KB/s   00:00    
scheduler.csr                                                                                              100% 1058     1.0MB/s   00:00    
scheduler-key.pem                                                                                          100% 1675     1.5MB/s   00:00    
scheduler.pem                                                                                              100% 1476     1.3MB/s   00:00    
admin.kubeconfig                                                                                           100% 6364     2.9MB/s   00:00    
controller-manager.kubeconfig                                                                              100% 6520     2.2MB/s   00:00    
scheduler.kubeconfig                                                                                       100% 6444     3.5MB/s   00:00


# 验证文件数量
[root@master01 pki]# for i in master0{2,3}; do echo $i; ssh $i "ls /etc/kubernetes/ssl |wc -l && ls /etc/kubernetes/*.kubeconfig |wc -l"; done
master02
23
3
master03
23
3
```



### 6.部署高可用组件

#### 6.1）master安装haproxy+keepalived

> [!CAUTION]
>
> **温馨提示:**
>
>    - <span style="color: red;">对于高可用组件，其实我们也可以单独找两台虚拟机来部署，但我为了节省2台机器，就直接在master节点复用了</span>
>    - <span style="color: red;">如果在云上安装K8S则无安装高可用组件了，毕竟公有云大部分都是不支持keepalived的，可以直接使用云产品，比如阿里的"SLB"，腾讯的"ELB"等SAAS产品</span>
>    - <span style="color: red;">推荐使用ELB，SLB有回环的问题，也就是SLB代理的服务器不能反向访问SLB，但是腾讯云修复了这个问题;</span>;

```bash
yum install -y keepalived haproxy
```

#### 6.2）master节点配置haproxy

> [!CAUTION]
>
> **温馨提示:**
>
>    - <span style="color: red;">haproxy的负载均衡器监听地址我配置是8443，你可以修改为其他端口，haproxy会用来反向代理各个master组件的地址;</span>
>    - <span style="color: red;">如果你真的修改请一定注意上面的证书配置的kubeconfig文件，也要一起修改，否则就会出现链接集群失败的问题</span>;

##### 6.2.1 备份配置文件

```bash
cp /etc/haproxy/haproxy.cfg{,`date +%F`}

[root@master01 haproxy]# ll /etc/haproxy/
total 8
-rw-r--r-- 1 root root 3142 Nov  4  2020 haproxy.cfg
-rw-r--r-- 1 root root 3142 Nov  2 14:23 haproxy.cfg2024-11-02
```

##### 6.2.2 修改配置文件

```bash
#所有节点的配置文件内容相同
cat > /etc/haproxy/haproxy.cfg <<'EOF'
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-haproxy
  bind *:33305
  mode http
  option httplog
  monitor-uri /ayouok

frontend k8s
  bind 0.0.0.0:8443
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s

backend k8s
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server master01   192.168.0.50:6443  check
  server master02   192.168.0.51:6443  check
  server master03   192.168.0.52:6443  check
EOF
-----------------------------------------------------------------------------------------------------------------------------
#注解
global
# maxconn 2000：设置最大连接数为 2000，表示 HAProxy 可以同时处理的最大连接数。
# ulimit-n 16384：设置文件描述符的限制为 16384，确保 HAProxy 可以打开足够的文件。
# log 127.0.0.1 local0 err：配置日志记录，将错误信息发送到本地的 syslog（本例中使用 local0）。
# stats timeout 30s：配置 HAProxy 的统计信息页面的超时时间为 30 秒。

defaults 块
# log global：继承全局日志设置。
# mode http：设置默认的操作模式为 HTTP（用于 HTTP 负载均衡）。
# option httplog：启用 HTTP 日志格式。
# timeout connect 5000：连接超时设置为 5000 毫秒（5 秒）。
# timeout client 50000：客户端超时设置为 50000 毫秒（50 秒）。
# timeout server 50000：服务器超时设置为 50000 毫秒（50 秒）。
# timeout http-request 15s：HTTP 请求超时设置为 15 秒。
# timeout http-keep-alive 15s：HTTP Keep-Alive 超时设置为 15 秒。

frontend monitor-haproxy 块
# frontend monitor-haproxy：定义前端配置，用于处理来自客户端的请求。
# *bind :33305：监听所有网络接口的 33305 端口。
# mode http：该前端以 HTTP 模式工作。
# option httplog：启用 HTTP 日志格式。
# monitor-uri /ayouok：定义一个用于监控的 URI，可以用于健康检查或查看 HAProxy 状态。

frontend k8s 块
# frontend k8s：处理来自客户端的请求，配置 Kubernetes 相关的负载均衡。
# bind 0.0.0.0:8443：监听所有网络接口的 8443 端口。
# mode tcp：以 TCP 模式工作，适用于非 HTTP 流量。
# option tcplog：启用 TCP 日志格式。
# tcp-request inspect-delay 5s：设置 TCP 请求的检查延迟为 5 秒，允许 HAProxy 在处理请求前进行一定的检查。
# default_backend k8s：指定默认后端为 k8s

backend k8s 块
# backend k8s：定义后端服务器组。
# mode tcp：设置为 TCP 模式。
# option tcplog：启用 TCP 日志格式。
# option tcp-check：启用 TCP 健康检查。
# balance roundrobin：使用轮询算法进行负载均衡。
# default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100：
	# inter 10s：设置健康检查间隔为 10 秒。
	# downinter 5s：如果服务器不可用，5 秒后再检查。
	# rise 2：连续 2 次成功的健康检查后，服务器被视为可用。
	# fall 2：连续 2 次失败的健康检查后，服务器被视为不可用。
	# slowstart 60s：新启动的服务器在 60 秒内逐渐增加处理能力。
	# maxconn 250：最大连接数为 250。
	# maxqueue 256：最大排队数为 256。
	# weight 100：权重设置为 100，影响负载均衡算法的流量分配。
# server master01 192.168.0.50:6443 check：定义后端服务器，IP 地址和端口，并启用健康检查。其他服务器（master02 和 master03）类似。
```

#### 6.3）master节点配置keepalived

##### 6.3.1 备份配置文件

```bash
cp /etc/keepalived/keepalived.conf{,`date +%F`}

[root@master01 haproxy]# ll /etc/keepalived/
total 8
-rw-r--r-- 1 root root 3142 Nov  4  2020 haproxy.cfg
-rw-r--r-- 1 root root 3142 Nov  2 14:23 haproxy.cfg2024-11-02
```

##### 6.3.2 master01修改配置文件

```bash
cat > /etc/keepalived/keepalived.conf <<'EOF'
global_defs {
   router_id 192.168.0.50  # router_id：设置 Keepalived 实例的唯一标识符，用于区分不同的节点。
}

vrrp_script chk_nginx {
    script "/etc/keepalived/check_port.sh 8443"  # script：指定用于检查端口的脚本，此处检查端口 8443 是否可用。
    interval 2  # interval：脚本执行的间隔时间为 2 秒。
    weight -20  # weight：如果脚本检测失败，降低当前节点的优先级 20 分，影响 VRRP 选举。
}

vrrp_instance VI_1 {
    state MASTER  # state：设置节点为 MASTER（主节点），处理虚拟 IP（VIP）流量。
    interface ens33  # interface：指定用于 VRRP 通信的网络接口，此处为 ens33。
    virtual_router_id 251  # virtual_router_id：分配一个唯一的虚拟路由器 ID（251）。
    priority 100  # priority：设置优先级为 100，较高的优先级让该节点更可能成为主节点。
    advert_int 1  # advert_int：设置 VRRP 广播间隔时间为 1 秒，频繁更新节点状态。
    mcast_src_ip 192.168.0.50  # mcast_src_ip：指定 VRRP 多播源 IP 地址为 192.168.0.50。
    nopreempt  # nopreempt：禁用抢占功能，主节点恢复时不会自动抢回虚拟 IP。
    authentication {
        auth_type PASS  # auth_type：设置认证方式为简单密码认证。
        auth_pass k8spasswd666  # auth_pass：设置认证密码为 "k8spasswd666"。
    }
    track_script {
         chk_nginx  # track_script：跟踪 "chk_nginx" 脚本的状态，若失败则降低当前节点的优先级。
    }
    virtual_ipaddress {
        192.168.0.40  # virtual_ipaddress：定义虚拟 IP 地址为 192.168.0.40，当主节点故障时，备份节点会接管此 IP。
    }
}
EOF
```

##### 6.3.3 master02修改配置文件

```bash
cat > /etc/keepalived/keepalived.conf <<'EOF'
global_defs {
   router_id 192.168.0.51  # master02 的 router_id 设置为 192.168.0.51
}

vrrp_script chk_nginx {
    script "/etc/keepalived/check_port.sh 8443"  # 检查端口 8443 的脚本
    interval 2  # 检查间隔 2 秒
    weight -20  # 如果失败，降低优先级 20
}

vrrp_instance VI_1 {
    state BACKUP  # 设置为备份节点
    interface ens33  # 配置网络接口 ens33
    virtual_router_id 251  # 配置虚拟路由器 ID
    priority 90  # 优先级 90
    advert_int 1  # 广播间隔 1 秒
    mcast_src_ip 192.168.0.51  # 设置源 IP 为 master02 的 IP
    nopreempt  # nopreempt：禁用抢占功能，主节点恢复时不会自动抢回虚拟 IP。
    authentication {
        auth_type PASS  # 简单密码认证
        auth_pass k8spasswd666  # 配置认证密码
    }
    track_script {
         chk_nginx  # 跟踪健康检查脚本
    }
    virtual_ipaddress {
        192.168.0.40  # 配置虚拟 IP
    }
}
EOF
```

##### 6.3.4 master03修改配置文件

```bash
cat > /etc/keepalived/keepalived.conf <<'EOF'
global_defs {
   router_id 192.168.0.52  # master03 的 router_id 设置为 192.168.0.52
}

vrrp_script chk_nginx {
    script "/etc/keepalived/check_port.sh 8443"  # 检查端口 8443 的脚本
    interval 2  # 检查间隔 2 秒
    weight -20  # 如果失败，降低优先级 20
}

vrrp_instance VI_1 {
    state BACKUP  # 设置为备份节点
    interface ens33  # 配置网络接口 ens33
    virtual_router_id 251  # 配置虚拟路由器 ID
    priority 90  # 优先级 90
    advert_int 1  # 广播间隔 1 秒
    mcast_src_ip 192.168.0.52  # 设置源 IP 为 master03 的 IP
    nopreempt  # nopreempt：禁用抢占功能，主节点恢复时不会自动抢回虚拟 IP。
    authentication {
        auth_type PASS  # 简单密码认证
        auth_pass k8spasswd666  # 配置认证密码
    }
    track_script {
         chk_nginx  # 跟踪健康检查脚本
    }
    virtual_ipaddress {
        192.168.0.40  # 配置虚拟 IP
    }
}
EOF
```

##### 6.3.5 添加检查端口检查脚本

```bash
cat > /etc/keepalived/check_port.sh << EOF
CHK_PORT=$1

if [ -n "$CHK_PORT" ]; then
    ss -lt | grep -q "$CHK_PORT"
    if [ $? -ne 0 ]; then
        echo "Port $CHK_PORT is not used, End."
        exit 1
    fi
else
    echo "Check port can't be empty!"
fi
EOF
```

#### 6.4）启动Haproxy、keepalived服务

##### 6.4.1 启动服务

```bash
#启动keepadlive 并设置开机自启
systemctl daemon-reload
systemctl enable keepalived --now

#启动haproxy 并设置开机自启
systemctl enable haproxy --now
```

##### 6.4.2 验证效果

![image-20241106225318562](./images/2.%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/image-20241106225318562-1732758436161-6-1736838949834-36.png)

```bash
[root@master01 ~]# ip a |grep 40
    inet 192.168.0.40/32 scope global ens33
[root@master01 ~]# ping -w1 192.168.0.40
PING 192.168.0.40 (192.168.0.40) 56(84) bytes of data.
64 bytes from 192.168.0.40: icmp_seq=1 ttl=64 time=0.139 ms

--- 192.168.0.40 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.139/0.139/0.139/0.000 ms
[root@master01 ~]# systemctl stop keepalived.service 

#这里发现 IP 已经漂移到其他的集群上去了，但是网络依然是通的
[root@master01 ~]# ip a |grep 40
[root@master01 ~]# ping -w1 192.168.0.40
PING 192.168.0.40 (192.168.0.40) 56(84) bytes of data.
64 bytes from 192.168.0.40: icmp_seq=1 ttl=64 time=1.68 ms

--- 192.168.0.40 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 1.685/1.685/1.685/0.000 ms


# 访问前面自定义的 monitor-haproxy 监控页面
[root@master01 ~]# curl http://192.168.0.40:33305/ayouok
<html><body><h1>200 OK</h1>
Service ready.
</body></html>
You have new mail in /var/spool/mail/root
[root@master02 ~]# curl http://192.168.0.40:33305/ayouok
<html><body><h1>200 OK</h1>
Service ready.
</body></html>       
[root@master03 ~]# curl http://192.168.0.40:33305/ayouok
<html><body><h1>200 OK</h1>
Service ready.
</body></html>
```

### 7.启动etcd集群

#### 7.1）创建etcd集群各节点配置文件

**配置参数解释**

```bash
# 配置etcd集群的名称和节点设置
name: 'master01'  # 当前etcd节点的名称，必须在整个集群中唯一

# 数据目录，etcd存储数据和日志的位置
data-dir: /var/lib/etcd  # 存储etcd数据的目录，etcd的所有数据库文件都存储在此目录

# 写时日志目录，用于存储ETCD的写操作日志
wal-dir: /var/lib/etcd/wal  # 写时日志目录，存储etcd的WAL（Write Ahead Log）文件

# 配置快照的间隔，超过该数量后会进行快照
snapshot-count: 5000  # 每写入5000条数据时，进行一次快照

# 配置心跳间隔时间，单位是毫秒
heartbeat-interval: 100  # 节点之间的心跳时间间隔，100毫秒

# 配置选举超时时间，单位是毫秒
election-timeout: 1000  # 集群成员进行leader选举时的超时时间，1000毫秒

# 配置后端数据库的最大字节数，0表示不限制
quota-backend-bytes: 0  # 数据库的最大大小，0表示没有限制

# 配置etcd节点监听的peer通讯URL
listen-peer-urls: 'https://192.168.0.50:2380'  # 节点间互相通信的地址，etcd peer节点之间的连接地址

# 配置etcd客户端访问URL，客户端通过这些URL连接etcd
listen-client-urls: 'https://192.168.0.50:2379,http://127.0.0.1:2379'  # 客户端连接etcd的URL，既支持HTTPS也支持HTTP

# 设置etcd允许的最大快照数量，超过会删除最旧的快照
max-snapshots: 3  # 最多保留3个快照文件

# 设置etcd允许的最大WAL（写日志）文件数量，超过会删除最旧的WAL文件
max-wals: 5  # 最多保留5个WAL文件

# CORS (跨域资源共享) 配置，目前没有配置项
cors:

# 配置集群中peer的初始广告URL，其他节点通过此URL发现此节点
initial-advertise-peer-urls: 'https://192.168.0.50:2380'  # 初始对等节点广告URL

# 配置客户端连接时广告的URL
advertise-client-urls: 'https://192.168.0.50:2379'  # 客户端访问时显示的etcd节点的URL

# 配置etcd集群的发现和集群初始化
discovery:
  discovery-fallback: 'proxy'  # 如果etcd发现机制失败，使用代理进行发现
  discovery-proxy:  # 代理配置（留空，表示不使用代理）
  discovery-srv:  # DNS服务发现（留空，表示不使用）

# 配置etcd集群的初始成员和初始集群状态
initial-cluster: 'master01=https://192.168.0.50:2380,master02=https://192.168.0.51:2380,master03=https://192.168.0.52:2380'  # 集群中其他节点的URL列表

initial-cluster-token: 'etcd-k8s-cluster'  # 集群的标识符，用于防止不同集群之间的重叠

initial-cluster-state: 'new'  # 配置集群状态为“新”，表示初始化一个新集群

# 禁用集群重新配置检查
strict-reconfig-check: false  # 禁止严格检查集群重新配置

# 启用etcd的v2 API
enable-v2: true  # 启用v2版本API

# 启用pprof性能分析功能
enable-pprof: true  # 启用pprof功能，用于性能分析

# 配置etcd为代理模式，关闭代理功能
proxy: 'off'  # 关闭代理模式

# 配置代理的失败等待时间（仅在代理模式下有效）
proxy-failure-wait: 5000  # 代理失败时的等待时间，单位毫秒

# 配置代理刷新间隔时间（仅在代理模式下有效）
proxy-refresh-interval: 30000  # 代理的刷新间隔时间，单位毫秒

# 配置代理连接超时时间（仅在代理模式下有效）
proxy-dial-timeout: 1000  # 代理连接超时时间，单位毫秒

# 配置代理写操作超时时间（仅在代理模式下有效）
proxy-write-timeout: 5000  # 代理写操作超时时间，单位毫秒

# 配置代理读取超时时间（仅在代理模式下有效）
proxy-read-timeout: 0  # 代理读取超时时间，0表示无超时限制

# 配置客户端传输安全性，启用TLS和认证
client-transport-security:
  cert-file: '/certs/etcd/etcd-server.pem'  # 客户端连接使用的证书文件路径
  key-file: '/certs/etcd/etcd-server-key.pem'  # 客户端连接使用的证书私钥文件路径
  client-cert-auth: true  # 启用客户端证书认证
  trusted-ca-file: '/certs/etcd/etcd-ca.pem'  # 配置信任的CA证书路径
  auto-tls: true  # 自动启用TLS加密

# 配置节点间的传输安全性，启用TLS和认证
peer-transport-security:
  cert-file: '/certs/etcd/etcd-server.pem'  # 对等节点间通信使用的证书文件路径
  key-file: '/certs/etcd/etcd-server-key.pem'  # 对等节点间通信使用的证书私钥文件路径
  peer-client-cert-auth: true  # 启用对等节点的证书认证
  trusted-ca-file: '/certs/etcd/etcd-ca.pem'  # 配置信任的CA证书路径
  auto-tls: true  # 自动启用TLS加密

# 配置调试模式，禁用调试
debug: false  # 关闭调试日志

# 配置日志包的日志级别（默认输出）
log-package-levels:

# 配置日志输出目标，这里使用默认日志输出
log-outputs: [default]  # 默认日志输出

# 强制新建集群，如果为true，则不允许恢复集群
force-new-cluster: false  # 如果为true，则强制启动一个新的etcd集群

----------------------------------------------------------------------------------------------------------------------------
# name：定义节点名称，标识该节点在集群中的角色。
# data-dir 和 wal-dir：数据目录和WAL日志目录，指定etcd持久化存储的位置。
# heartbeat-interval 和 election-timeout：这两个参数控制etcd的心跳和选举超时时间，影响集群稳定性和性能。
# listen-peer-urls 和 listen-client-urls：分别定义etcd节点监听的对等通信端口和客户端访问端口。
# max-snapshots 和 max-wals：定义etcd保留的快照和WAL日志文件的数量，超过该数量会删除最旧的文件。
# initial-advertise-peer-urls 和 advertise-client-urls：定义节点对外暴露的广告URL，用于其他节点或客户端发现该节点。
# initial-cluster：指定集群中所有节点的初始配置信息。
# client-transport-security 和 peer-transport-security：配置TLS/SSL加密和认证，增强etcd集群的安全性。
# debug：控制是否启用调试日志。
# force-new-cluster：决定是否强制初始化一个新的etcd集群。
```

##### 7.1.1 master01配置文件

```bash
cat > /etc/etcd/etcd.config.yml <<'EOF'
name: 'master01'
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: 'https://192.168.0.50:2380'
listen-client-urls: 'https://192.168.0.50:2379,http://127.0.0.1:2379'
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: 'https://192.168.0.50:2380'
advertise-client-urls: 'https://192.168.0.50:2379'
discovery:
discovery-fallback: 'proxy'
discovery-proxy:
discovery-srv:
initial-cluster: 'master01=https://192.168.0.50:2380,master02=https://192.168.0.51:2380,master03=https://192.168.0.52:2380'
initial-cluster-token: 'etcd-k8s-cluster'
initial-cluster-state: 'new'
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: 'off'
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: '/etc/etcd/ssl/etcd.pem'
  key-file: '/etc/etcd/ssl/etcd-key.pem'
  client-cert-auth: true
  trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem'
  auto-tls: true
peer-transport-security:
  cert-file: '/etc/etcd/ssl/etcd.pem'
  key-file: '/etc/etcd/ssl/etcd-key.pem'
  peer-client-cert-auth: true
  trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem'
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
EOF
```

##### 7.1.2 master02配置文件

```bash
cat > /etc/etcd/etcd.config.yml <<'EOF'
name: 'master02'
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: 'https://192.168.0.51:2380'
listen-client-urls: 'https://192.168.0.51:2379,http://127.0.0.1:2379'
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: 'https://192.168.0.51:2380'
advertise-client-urls: 'https://192.168.0.51:2379'
discovery:
discovery-fallback: 'proxy'
discovery-proxy:
discovery-srv:
initial-cluster: 'master01=https://192.168.0.50:2380,master02=https://192.168.0.51:2380,master03=https://192.168.0.52:2380'
initial-cluster-token: 'etcd-k8s-cluster'
initial-cluster-state: 'new'
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: 'off'
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: '/etc/etcd/ssl/etcd.pem'
  key-file: '/etc/etcd/ssl/etcd-key.pem'
  client-cert-auth: true
  trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem'
  auto-tls: true
peer-transport-security:
  cert-file: '/etc/etcd/ssl/etcd.pem'
  key-file: '/etc/etcd/ssl/etcd-key.pem'
  peer-client-cert-auth: true
  trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem'
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
EOF
```

##### 7.1.3 master03配置文件

```bash
cat > /etc/etcd/etcd.config.yml <<'EOF'
name: 'master03'
data-dir: /var/lib/etcd
wal-dir: /var/lib/etcd/wal
snapshot-count: 5000
heartbeat-interval: 100
election-timeout: 1000
quota-backend-bytes: 0
listen-peer-urls: 'https://192.168.0.52:2380'
listen-client-urls: 'https://192.168.0.52:2379,http://127.0.0.1:2379'
max-snapshots: 3
max-wals: 5
cors:
initial-advertise-peer-urls: 'https://192.168.0.52:2380'
advertise-client-urls: 'https://192.168.0.52:2379'
discovery:
discovery-fallback: 'proxy'
discovery-proxy:
discovery-srv:
initial-cluster: 'master01=https://192.168.0.50:2380,master02=https://192.168.0.51:2380,master03=https://192.168.0.52:2380'
initial-cluster-token: 'etcd-k8s-cluster'
initial-cluster-state: 'new'
strict-reconfig-check: false
enable-v2: true
enable-pprof: true
proxy: 'off'
proxy-failure-wait: 5000
proxy-refresh-interval: 30000
proxy-dial-timeout: 1000
proxy-write-timeout: 5000
proxy-read-timeout: 0
client-transport-security:
  cert-file: '/etc/etcd/ssl/etcd.pem'
  key-file: '/etc/etcd/ssl/etcd-key.pem'
  client-cert-auth: true
  trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem'
  auto-tls: true
peer-transport-security:
  cert-file: '/etc/etcd/ssl/etcd.pem'
  key-file: '/etc/etcd/ssl/etcd-key.pem'
  peer-client-cert-auth: true
  trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem'
  auto-tls: true
debug: false
log-package-levels:
log-outputs: [default]
force-new-cluster: false
EOF
```

#### 7.2）编写etcd启动脚本

```bash
cat > /usr/lib/systemd/system/etcd.service <<'EOF'
[Unit]
Description=Etcd Service
Documentation=https://coreos.com/etcd/docs/latest/
After=network.target

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yml
Restart=on-failure
RestartSec=10
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd3.service
EOF

# [Unit] 部分:
# Description: 服务的描述信息，简要说明该服务的功能或目的。
# Documentation: 提供该服务的官方文档链接，方便用户参考详细的使用和配置手册。
# After: 定义该服务的启动顺序，表示在网络服务（network.target）启动之后再启动该服务。

# [Service] 部分:
# Type: 服务的类型，`notify` 表示该服务在启动时会通知Systemd其状态，通常用于长时间启动的服务。
# ExecStart: 定义启动该服务时需要执行的命令和参数，指定了 `etcd` 二进制文件及其配置文件路径。
# Restart: 配置服务在失败时的重启策略，`on-failure` 表示服务在失败时会自动重启。
# RestartSec: 配置服务失败后重启之前的等待时间，`10` 表示等 10 秒后再尝试重启服务。
# LimitNOFILE: 设置该服务进程的文件描述符限制，`65536` 表示该服务最大可以同时打开 65536 个文件句柄。

# [Install] 部分:
# WantedBy: 指定该服务在哪个运行级别（target）下启动，`multi-user.target` 表示该服务在系统进入多用户模式时启动。
# Alias: 为该服务设置一个别名，`etcd3.service` 表示可以通过该别名来管理和控制服务。
```

#### 7.3）启动etcd服务

```bash
#1.启动集群并配置开机自启动
systemctl daemon-reload && systemctl enable --now etcd
[root@master01 ~]# systemctl status etcd
● etcd.service - Jason Yin's Etcd Service
   Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2024-11-07 10:40:22 CST; 19ms ago
     Docs: https://coreos.com/etcd/docs/latest/
 Main PID: 46471 (etcd)
    Tasks: 5
   Memory: 23.4M
   CGroup: /system.slice/etcd.service
           └─46471 /usr/local/bin/etcd --config-file=/softwares/etcd/etcd.config.yml

Nov 07 10:40:22 master01 etcd[46471]: {"level":"info","ts":"2024-11-07T10:40:22.739211+0800","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"c803656a4c148fcb became leader at term 2"}
Nov 07 10:40:22 master01 etcd[46471]: {"level":"info","ts":"2024-11-07T10:40:22.739227+0800","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: c803656a4c148fcb elected leader c803...fcb at term 2"}
Nov 07 10:40:22 master01 etcd[46471]: {"level":"warn","ts":"2024-11-07T10:40:22.767405+0800","caller":"etcdserver/cluster_util.go:288","msg":"failed to reach the peer URL","address":"https://192.168.0.52:2380/versio...
Nov 07 10:40:22 master01 etcd[46471]: {"level":"warn","ts":"2024-11-07T10:40:22.767704+0800","caller":"etcdserver/cluster_util.go:155","msg":"failed to get version","remote-member-id":"c6f329b48866d60...ction refused"}
Nov 07 10:40:22 master01 etcd[46471]: {"level":"info","ts":"2024-11-07T10:40:22.808724+0800","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"c803656a4c1...
Nov 07 10:40:22 master01 etcd[46471]: {"level":"info","ts":"2024-11-07T10:40:22.809105+0800","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
Nov 07 10:40:22 master01 systemd[1]: Started Jason Yin's Etcd Service.
Nov 07 10:40:22 master01 etcd[46471]: {"level":"info","ts":"2024-11-07T10:40:22.811995+0800","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
Nov 07 10:40:22 master01 etcd[46471]: {"level":"info","ts":"2024-11-07T10:40:22.812268+0800","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
Nov 07 10:40:22 master01 etcd[46471]: {"level":"info","ts":"2024-11-07T10:40:22.812413+0800","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}



#2.查看etcd集群状态
etcdctl --endpoints="192.168.0.50:2379,192.168.0.51:2379,192.168.0.52:2379" \
        --cacert=/etc/etcd/ssl/etcd-ca.pem \
        --cert=/etc/etcd/ssl/etcd.pem \
        --key=/etc/etcd/ssl/etcd-key.pem \
        endpoint status --write-out=table
        
[root@master01 network-scripts]# etcdctl --endpoints="192.168.0.50:2379,192.168.0.51:2379,192.168.0.52:2379" \
>         --cacert=/etc/etcd/ssl/etcd-ca.pem \
>         --cert=/etc/etcd/ssl/etcd.pem \
>         --key=/etc/etcd/ssl/etcd-key.pem \
>         endpoint status --write-out=table
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|     ENDPOINT      |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| 192.168.0.50:2379 | c803656a4c148fcb |  3.5.10 |   20 kB |     false |      false |         2 |          9 |                  9 |        |
| 192.168.0.51:2379 | fd69324f94aba0e6 |  3.5.10 |   20 kB |     false |      false |         2 |          9 |                  9 |        |
| 192.168.0.52:2379 | c6f329b48866d60f |  3.5.10 |   20 kB |      true |      false |         2 |          9 |                  9 |        |
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

etcdctl 命令行工具，连接到指定的 3 个 Etcd 节点（使用安全的 HTTPS 协议），并查询每个节点的状态信息，最后将结果以表格形式显示出来。    

# ENDPOINT：Etcd 节点的地址和端口。
# ID：节点的唯一 ID。
# VERSION：Etcd 版本。
# DB SIZE：数据库的大小。
# IS LEADER：是否是领导节点。
# IS LEARNER：表明节点是否是学习者节点。
# RAFT TERM：Raft 选举的任期。
# RAFT INDEX：Raft 日志的索引位置。
# RAFT APPLIED INDEX：已应用的 Raft 日志索引位置。
# ERRORS：节点运行过程中的错误信息。
```

#### 7.4）验证高可用效果

![image-20241107110637779](./images/2.%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/image-20241107110637779-1732758436160-5-1736838949834-34.png)

```bash
#查看集群信息
etcdctl --endpoints="192.168.0.50:2379,192.168.0.51:2379,192.168.0.52:2379" \
>         --cacert=/certs/etcd/etcd-ca.pem \
>         --cert=/certs/etcd/etcd-server.pem \
>         --key=/certs/etcd/etcd-server-key.pem \
>         endpoint status --write-out=table
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|     ENDPOINT      |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| 192.168.0.50:2379 | c803656a4c148fcb |  3.5.10 |   25 kB |      true |      false |         2 |          9 |                  9 |        |
| 192.168.0.51:2379 | fd69324f94aba0e6 |  3.5.10 |   20 kB |     false |      false |         2 |          9 |                  9 |        |
| 192.168.0.52:2379 | c6f329b48866d60f |  3.5.10 |   20 kB |     false |      false |         2 |          9 |                  9 |        |
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

#master01 停止etcd服务
systemctl stop etcd

#查看发现master02自动选举成为 leader
etcdctl --endpoints="192.168.0.50:2379,192.168.0.51:2379,192.168.0.52:2379"         --cacert=/certs/etcd/etcd-ca.pem         --cert=/certs/etcd/etcd-server.pem         --key=/certs/etcd/etcd-server-key.pem         endpoint status --write-out=table
{"level":"warn","ts":"2024-11-07T10:59:50.175421+0800","logger":"etcd-client","caller":"v3@v3.5.10/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000288e00/192.168.0.50:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \"transport: Error while dialing: dial tcp 192.168.0.50:2379: connect: connection refused\""}
Failed to get the status of endpoint 192.168.0.50:2379 (context deadline exceeded)
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|     ENDPOINT      |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| 192.168.0.51:2379 | fd69324f94aba0e6 |  3.5.10 |   20 kB |      true |      false |         3 |         10 |                 10 |        |
| 192.168.0.52:2379 | c6f329b48866d60f |  3.5.10 |   20 kB |     false |      false |         3 |         10 |                 10 |        |
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+


#启动master01 验证是否会抢占
systemctl start etcd
etcdctl --endpoints="192.168.0.50:2379,192.168.0.51:2379,192.168.0.52:2379"         --cacert=/certs/etcd/etcd-ca.pem         --cert=/certs/etcd/etcd-server.pem         --key=/certs/etcd/etcd-server-key.pem         endpoint status --write-out=table
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|     ENDPOINT      |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| 192.168.0.50:2379 | c803656a4c148fcb |  3.5.10 |   25 kB |     false |      false |         3 |         11 |                 11 |        |
| 192.168.0.51:2379 | fd69324f94aba0e6 |  3.5.10 |   20 kB |      true |      false |         3 |         11 |                 11 |        |
| 192.168.0.52:2379 | c6f329b48866d60f |  3.5.10 |   20 kB |     false |      false |         3 |         11 |                 11 |        |
+-------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
```



### 8.启动ApiServer组件

#### 8.1）创建ApiServer集群个节点的配置文件

**配置文件解释**

```bash
[Unit] 部分
这个部分主要是描述该服务的元数据和依赖关系。
Description: 这是该服务的简短描述，表明这个服务是用于运行 kube-apiserver（Kubernetes API Server）。
Description=Kubernetes API Server
这个字段通常用于在 systemctl 查询服务状态时显示该服务的简要信息。
Documentation: 提供了该服务的官方文档链接。通过访问该链接，用户可以进一步了解如何配置和管理 Kubernetes API Server。
Documentation=https://github.com/kubernetes/kubernetes
After: 定义服务启动顺序。这里的 After=network.target 表示 kube-apiserver 服务会在系统的网络服务启动后再启动。这确保了网络配置已经完成，API Server 可以与其他集群组件进行通信。
After=network.target


[Service] 部分
这个部分定义了如何启动和管理 kube-apiserver 服务。
ExecStart: 这是启动服务时要执行的命令。以下是传递给 kube-apiserver 的一系列参数：
--v=2：设置日志的详细程度。v=2 表示输出适中的日志信息。
--bind-address=0.0.0.0：设置 API Server 监听的网络接口地址，这里使用 0.0.0.0 表示监听所有可用的网络接口。
--secure-port=6443：指定 Kubernetes API Server 的 HTTPS 服务端口，默认为 6443。
--allow_privileged=true：允许启动特权模式的容器。
--advertise-address=192.168.0.50：指定对外宣传的 API Server 地址，集群其他节点和客户端可以通过此地址访问 API Server。
--service-cluster-ip-range=10.96.0.0/16：定义 Kubernetes 服务的 IP 地址段，提供了 1,048,576 个 IP 地址 供集群中的服务使用。
--service-node-port-range=30000-32767：定义 Kubernetes 服务使用的节点端口范围。
--etcd-servers=...：指定集群中使用的 etcd 服务地址，kube-apiserver 会连接这些地址来存储集群的配置信息。
TLS/认证相关参数：多项 --tls-*、--etcd-* 和 --client-ca-file 等参数配置了与 TLS 证书和客户端证书相关的内容，用于加密通信和验证身份。
--enable-admission-plugins=...：启用一组 Admission 控制插件，这些插件用于验证和控制进入集群的请求，如资源限制、权限控制等。
--authorization-mode=Node,RBAC：设置授权模式，Node 和 RBAC 允许对 API 请求进行节点授权和基于角色的访问控制。
--enable-bootstrap-token-auth=true：启用引导令牌认证，以支持新的节点加入集群时使用引导令牌进行认证。
--requestheader-*：这些参数配置了前端代理的请求头，以允许代理向 API Server 转发身份验证信息。
Restart: 定义服务的重启策略。on-failure 表示当服务崩溃或非正常退出时，系统会自动重启该服务。
Restart=on-failure
RestartSec: 定义服务崩溃后等待多长时间再尝试重启。这里设置为 10s，即 10 秒钟。
RestartSec=10s
LimitNOFILE: 设置该服务的文件描述符限制，65535 表示该服务最多能打开 65535 个文件句柄。这个设置通常适用于需要大量连接的服务。
LimitNOFILE=65535


[Install] 部分
这个部分定义了服务的安装信息，指定服务何时自动启动。
WantedBy: 指定服务启动的目标（运行级别）。multi-user.target 表示该服务会在系统进入多用户模式时自动启动，通常这是系统启动的默认模式。
WantedBy=multi-user.target
```

##### 8.1.1 master01配置文件

```bash
cat > /usr/lib/systemd/system/kube-apiserver.service << 'EOF'
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
	  --allow_privileged=true \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.0.50 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.0.50:2379,https://192.168.0.51:2379,https://192.168.0.52:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/ssl/k8s-ca.pem  \
      --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/ssl/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/ssl/sa.key \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
EOF
```

##### 8.1.2 master02配置文件

```bash
cat > /usr/lib/systemd/system/kube-apiserver.service << 'EOF'
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
	  --allow_privileged=true \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.0.51 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.0.50:2379,https://192.168.0.51:2379,https://192.168.0.52:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/ssl/k8s-ca.pem  \
      --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/ssl/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/ssl/sa.key \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
EOF
```

##### 8.1.3 master03配置文件

```bash
cat > /usr/lib/systemd/system/kube-apiserver.service << 'EOF'
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
      --v=2  \
	  --allow_privileged=true \
      --bind-address=0.0.0.0  \
      --secure-port=6443  \
      --advertise-address=192.168.0.52 \
      --service-cluster-ip-range=10.96.0.0/16  \
      --service-node-port-range=30000-32767  \
      --etcd-servers=https://192.168.0.50:2379,https://192.168.0.51:2379,https://192.168.0.52:2379 \
      --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem  \
      --etcd-certfile=/etc/etcd/ssl/etcd.pem  \
      --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem  \
      --client-ca-file=/etc/kubernetes/ssl/k8s-ca.pem  \
      --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem  \
      --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem  \
      --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem  \
      --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem  \
      --service-account-key-file=/etc/kubernetes/ssl/sa.pub  \
      --service-account-signing-key-file=/etc/kubernetes/ssl/sa.key \
      --service-account-issuer=https://kubernetes.default.svc.cluster.local \
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \
      --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \
      --authorization-mode=Node,RBAC  \
      --enable-bootstrap-token-auth=true  \
      --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem  \
      --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem  \
      --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem  \
      --requestheader-allowed-names=aggregator  \
      --requestheader-group-headers=X-Remote-Group  \
      --requestheader-extra-headers-prefix=X-Remote-Extra-  \
      --requestheader-username-headers=X-Remote-User

Restart=on-failure
RestartSec=10s
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target
EOF
```

#### 8.2）启动Apiserver服务

```bash
#重新加载systemd，启动apiserver并设置自启
systemctl daemon-reload && systemctl enable --now kube-apiserver
[root@master01 ~]# systemctl status kube-apiserver
● kube-apiserver.service - Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: active (running) since Mon 2024-11-18 22:42:47 CST; 10h ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 6865 (kube-apiserver)
   CGroup: /system.slice/kube-apiserver.service
           └─6865 /usr/local/bin/kube-apiserver --v=2 --bind-address=0.0.0.0 --secure-port=6443 --allow_privileged=true --advertise-address=192.168.0.50 --service-cluster-ip-range=10.96.0.0/16 --service-node-port-range=30000-32767 --e...

Nov 19 08:43:30 master01 kube-apiserver[6865]: I1119 08:43:30.493735    6865 cacher.go:465] cacher (rolebindings.rbac.authorization.k8s.io): initialized
Nov 19 08:43:30 master01 kube-apiserver[6865]: I1119 08:43:30.499864    6865 trace.go:236] Trace[784609540]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7dea1a7a-bcc6-4b89-922f-a3b46433a187,client:::1,protocol:HTTP...
Nov 19 08:43:30 master01 kube-apiserver[6865]: Trace[784609540]: ["cacher list" audit-id:7dea1a7a-bcc6-4b89-922f-a3b46433a187,type:clusterroles.rbac.authorization.k8s.io 11476ms (08:43:19.023)
Nov 19 08:43:30 master01 kube-apiserver[6865]: Trace[784609540]:  ---"Ready" 11468ms (08:43:30.491)]
Nov 19 08:43:30 master01 kube-apiserver[6865]: Trace[784609540]: [11.476439644s] [11.476439644s] END
Nov 19 08:43:30 master01 kube-apiserver[6865]: I1119 08:43:30.510325    6865 trace.go:236] Trace[456175327]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.0.0/tools/cache/reflector.go:229 (19-Nov-2024 08:43:19.022)... time: 11487ms):
Nov 19 08:43:30 master01 kube-apiserver[6865]: Trace[456175327]: ---"Objects listed" error:<nil> 11487ms (08:43:30.510)
Nov 19 08:43:30 master01 kube-apiserver[6865]: Trace[456175327]: [11.487632438s] [11.487632438s] END
Nov 19 08:52:38 master01 kube-apiserver[6865]: I1119 08:52:38.779151    6865 apf_controller.go:455] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0018453098959216754 seatDemandStdev=0.042917417528431145 ...
Nov 19 08:52:58 master01 kube-apiserver[6865]: I1119 08:52:58.780424    6865 apf_controller.go:455] "Update CurrentCL" plName="exempt" seatDemandHighWatermark=1 seatDemandAvg=0.0036608706586515843 seatDemandStdev=0.06039427691985565 s...
Hint: Some lines were ellipsized, use -l to show in full.
```

### 9.启动controller-manager

#### 9.1）master节点修改配置文件

```bash
cat > /usr/lib/systemd/system/kube-controller-manager.service << 'EOF'
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
      --v=2 \
      --root-ca-file=/etc/kubernetes/ssl/k8s-ca.pem \
      --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-ca.pem \
      --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-ca-key.pem \
      --service-account-private-key-file=/etc/kubernetes/ssl/sa.key \
      --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \
      --leader-elect=true \
      --use-service-account-credentials=true \
      --node-monitor-grace-period=40s \
      --node-monitor-period=5s \
      --controllers=*,bootstrapsigner,tokencleaner \
      --allocate-node-cidrs=true \
      --cluster-cidr=172.16.0.0/16 \
      --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem \
      --node-cidr-mask-size=24

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
EOF
-----------------------------------------------------------------------------------------------------------------------------
#注解
# [Unit]
# Description：服务的描述信息，这里说明这个服务是 "Kubernetes Controller Manager"。
# Documentation：指向相关文档的链接。
# After=network.target：指定此服务在网络目标成功激活后启动。
# 
# plaintext
# [Service]
# ExecStart：定义启动服务的具体命令。
# --v=2：设置日志详细级别为2，提供足够的信息以便于故障排查。
# --root-ca-file：指定用于其他签名生成的根 CA 文件。
# --cluster-signing-cert-file：指明用于默认自签名证书的证书文件。
# --cluster-signing-key-file：指明用于默认自签名的密钥文件。
# --service-account-private-key-file：Kubernetes用于签名服务账户令牌的私钥文件。
# --kubeconfig：指定 kube-controller-manager 的 kubeconfig 文件路径。
# --leader-elect=true：启用leader选举，确保高可用性。
# --use-service-account-credentials=true：指定是否使用服务账户凭证。
# --node-monitor-grace-period=40s：节点状态从Ready变成NotReady的宽限期。
# --node-monitor-period=5s：检查节点状态的频率。
# --controllers=*,bootstrapsigner,tokencleaner：启动默认的控制器和额外的 bootstrapsigner 和 tokencleaner。
# --allocate-node-cidrs=true：为每个节点分配IP段。
# --cluster-cidr=10.96.0.0/12：用于分配给Pod的CIDR范围。
# --requestheader-client-ca-file：用于apiserver集群对前端请求的CA证书。
# --node-cidr-mask-size=24：为各个节点分配的IP地址块大小。
# plaintext
# Restart=always
# RestartSec=10s
# Restart=always：无论何种故障，服务都将始终重新启动。
# RestartSec=10s：在失败后重新启动服务之前的等待时间。
# plaintext
# 
# [Install]
# WantedBy=multi-user.target：表示该服务将在多用户系统运行级别(即常规系统操作时间)启动。
```

#### 9.2）启动controller-manager服务

```nsis
systemctl daemon-reload && systemctl enable --now kube-controller-manager
[root@master01 ~]# systemctl status kube-controller-manager
● kube-controller-manager.service - Kubernetes Controller Manager
   Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2024-11-07 15:43:22 CST; 4s ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 50030 (kube-controller)
    Tasks: 4
   Memory: 20.6M
   CGroup: /system.slice/kube-controller-manager.service
           └─50030 /usr/local/bin/kube-controller-manager --v=2 --root-ca-file=/certs/kubernetes/k8s-ca.pem --cluster-signing-cert-file=/certs/kubernetes/k8s-ca.pem --cluster-signing-key-file=/certs/kubernetes/k8s-c...

Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.259880   50030 horizontal.go:200] "Starting HPA controller"
Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.259891   50030 shared_informer.go:311] Waiting for caches to sync for HPA
Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.399607   50030 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.399657   50030 controllermanager.go:613] "Starting controller" controller="deployment-controller"
Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.399795   50030 serviceaccounts_controller.go:111] "Starting service account controller"
Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.399811   50030 shared_informer.go:311] Waiting for caches to sync for service account
Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.553786   50030 controllermanager.go:642] "Started controller" controller="deployment-controller"
Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.554045   50030 controllermanager.go:613] "Starting controller" controller="node-ipam-controller"
Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.554291   50030 deployment_controller.go:168] "Starting controller" controller="deployment"
Nov 07 15:43:24 master01 kube-controller-manager[50030]: I1107 15:43:24.554304   50030 shared_informer.go:311] Waiting for caches to sync for deployment
```

### 10.启动Scheduler

#### 10.1）master节点修改配置文件

```bash
cat > /usr/lib/systemd/system/kube-scheduler.service <<'EOF'
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
      --v=2 \
      --leader-elect=true \
      --kubeconfig=/etc/kubernetes/scheduler.kubeconfig

Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
EOF
-----------------------------------------------------------------------------------------------------------------------------
#注解
# [Unit]
# Description：服务的描述信息，这里说明这个服务是 "Kubernetes Scheduler"。
# Documentation：指向相关文档的链接。
# After=network.target：指定此服务在网络目标成功激活后启动。

# [Service]
# ExecStart：定义启动服务的具体命令。
# --v=2：设置日志详细级别为2，提供足够的信息以便于故障排查。
# --leader-elect=true：启用leader选举，这确保了在高可用性环境中只有一个scheduler实例在工作。
# --kubeconfig：指定 kube-scheduler 的 kubeconfig 文件路径，用于连接和认证集群。

# Restart=always：无论何种故障，服务都将始终重新启动。
# RestartSec=10s：在失败后重新启动服务之前的等待时间。

# [Install]
# WantedBy=multi-user.target：表示该服务将在多用户系统运行级别(即常规系统操作时间)启动。
```

#### 10.2）启动controller-manager服务

```nsis
systemctl daemon-reload && systemctl enable --now kube-scheduler
[root@master01 ~]# systemctl  status kube-scheduler
● kube-scheduler.service - Kubernetes Scheduler
   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2024-11-07 16:06:38 CST; 751ms ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 50332 (kube-scheduler)
    Tasks: 6
   Memory: 12.4M
   CGroup: /system.slice/kube-scheduler.service
           └─50332 /usr/local/bin/kube-scheduler --v=2 --leader-elect=true --kubeconfig=/certs/kubeconfig/kube-scheduler.kubeconfig

Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.326842   50332 flags.go:64] FLAG: --tls-cert-file=""
Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.326847   50332 flags.go:64] FLAG: --tls-cipher-suites="[]"
Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.326860   50332 flags.go:64] FLAG: --tls-min-version=""
Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.327056   50332 flags.go:64] FLAG: --tls-private-key-file=""
Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.327075   50332 flags.go:64] FLAG: --tls-sni-cert-key="[]"
Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.327156   50332 flags.go:64] FLAG: --v="2"
Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.327172   50332 flags.go:64] FLAG: --version="false"
Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.327184   50332 flags.go:64] FLAG: --vmodule=""
Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.327194   50332 flags.go:64] FLAG: --write-config-to=""
Nov 07 16:06:38 master01 kube-scheduler[50332]: I1107 16:06:38.574484   50332 serving.go:348] Generated self-signed cert in-memory
```

### 11.创建Bootstrapping自动颁发kubelet证书配置

#### 11.1）master01创建bootstrap-kubelet配置文件

##### 11.1.1 设置集群

```bash
#配置并初始化 Kubernetes 集群的信息
kubectl config set-cluster Kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/k8s-ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.40:8443 \
  --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
  
  
# set-cluster Kubernetes: 为新的集群配置命名为 k8s
# --certificate-authority: 指定 CA 证书路径，用于验证集群的安全连接
# --embed-certs=true: 将 CA 证书嵌入到 kubeconfig 中，确保 kubeconfig 可独立使用
# --server: 定义 API 服务器的地址和端口
# --kubeconfig: 指定要修改的 kubeconfig 文件。  
```

##### 11.1.2 创建用户

```bash
#为集群认证配置一个新用户
kubectl config set-credentials tls-bootstrap-token-user  \
  --token=c8ad9c.2e4d610cf3e7426e \
  --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
  
# set-credentials tls-bootstrap-token-user: 创建或更新名为 tls-bootstrap-token-user 的用户认证信息
# --token: 指定用于身份验证的令牌
# --kubeconfig: 目标 kubeconfig 文件路径。
```

##### 11.1.3 将集群和用户进行绑定

```bash
#创建并配置上下文，将特定用户与集群关联
kubectl config set-context tls-bootstrap-token-user@Kubernetes \
  --cluster=Kubernetes \
  --user=tls-bootstrap-token-user \
  --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
  
# set-credentials tls-bootstrap-token-user: 创建或更新名为 tls-bootstrap-token-user 的用户认证信息
# --token: 指定用于身份验证的令牌
# --kubeconfig: 目标 kubeconfig 文件路径。 	  
```

##### 11.1.4 配置默认的上下文

```bash
#设置默认使用的上下文
kubectl config use-context tls-bootstrap-token-user@Kubernetes \
  --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig
  
# use-context tls-bootstrap-token-user@Kubernetes: 将指定上下文设为默认，以便后续的 kubectl 操作使用。
# --kubeconfig: 指定操作的 kubeconfig 文件。  
```



#### 11.2）所有master节点拷贝管理证书

```bash
1.所有master都拷贝管理员的证书文件    
mkdir -p /root/.kube
cp /etc/kubernetes/admin.kubeconfig /root/.kube/config
2.查看master组件    
[root@master01 ~]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
etcd-0               Healthy   ok        
controller-manager   Healthy   ok        
scheduler            Healthy   ok  
    

3.查看集群状态，如果未来cs组件移除了也没关系，我们可以使用"cluster-info"子命令查看集群状态
[root@master01 ~]# kubectl cluster-info 
Kubernetes control plane is running at https://192.168.0.40:8443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

#### 11.3）master01 创建bootstrap-secret授权

##### 11.3.1 创建配bootstrap-secret文件用于授权

```yaml
cat > bootstrap-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-token-c8ad9c
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  description: "The default bootstrap token generated by 'kubelet '."
  token-id: c8ad9c
  token-secret: 2e4d610cf3e7426e
  usage-bootstrap-authentication: "true"
  usage-bootstrap-signing: "true"
  auth-extra-groups:  system:bootstrappers:default-node-token,system:bootstrappers:worker,system:bootstrappers:ingress
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubelet-bootstrap
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node-bootstrapper
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:bootstrappers:default-node-token
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-autoapprove-bootstrap
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:bootstrappers:default-node-token
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: node-autoapprove-certificate-rotation
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:nodes
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kube-apiserver
EOF
------------------------------------------------------------------------------------------------------------------------------------------------------
# 以下都为注释


# apiVersion: v1
# kind: Secret
# metadata:
#   name: bootstrap-token-10001      # 该 Secret 的名称为 bootstrap-token-10001,通常以bootstrap-token-为前缀，后接令牌ID作为名称
#   namespace: kube-system            # Secret 位于 kube-system 命名空间中
# type: bootstrap.kubernetes.io/token # 指定该 Secret 是一个用于集群节点引导认证的 bootstrap token 类型
# stringData:
#   description: "The default bootstrap token generated by 'kubelet '." # 说明该 token 是由 kubelet 生成的默认引导 token
#   token-id: 10001                  # 该 token 的唯一标识符 (token-id),实际用户的密码要配置为 10001.be31df5b85f4f55404b96bffe768562a
#   token-secret: be31df5b85f4f55404b96bffe768562a    # 该 token 对应的密钥 (token-secret)
#   usage-bootstrap-authentication: "true"  # 设置该 token 用于引导认证，表示允许其用于节点引导认证
#   usage-bootstrap-signing: "true"        # 设置该 token 用于签名，表示该 token 可用于签署请求
#   auth-extra-groups:  system:bootstrappers:bootstrap-token-10001,system:bootstrappers:worker,system:bootstrappers:ingress # 为该 token 指定附加的授权组，允许与该 token 关联的用户或服务属于这些组
# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   name: kubelet-bootstrap         # ClusterRoleBinding 的名称是 kubelet-bootstrap
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: system:node-bootstrapper  # 引用一个预定义的 ClusterRole：system:node-bootstrapper，授予集群节点引导权限
# subjects:
# - apiGroup: rbac.authorization.k8s.io
#   kind: Group
#   name: system:bootstrappers:bootstrap-token-10001 # 将该权限授予属于 "system:bootstrappers:bootstrap-token-10001" 组的用户或服务
# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   name: node-autoapprove-bootstrap  # ClusterRoleBinding 的名称是 node-autoapprove-bootstrap
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient # 引用一个预定义的 ClusterRole：system:certificates.k8s.io:certificatesigningrequests:nodeclient，允许节点客户端自动批准证书签名请求
# subjects:
# - apiGroup: rbac.authorization.k8s.io
#   kind: Group
#   name: system:bootstrappers:bootstrap-token-10001 # 将该权限授予属于 "system:bootstrappers:bootstrap-token-10001" 组的用户或服务
# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   name: node-autoapprove-certificate-rotation # ClusterRoleBinding 的名称是 node-autoapprove-certificate-rotation
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient # 引用一个预定义的 ClusterRole：system:certificates.k8s.io:certificatesigningrequests:selfnodeclient，用于自我签名的证书自动批准
# subjects:
# - apiGroup: rbac.authorization.k8s.io
#   kind: Group
#   name: system:nodes # 将该权限授予属于 "system:nodes" 组的用户或服务，该组代表集群中的节点
# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRole
# metadata:
#   annotations:
#     rbac.authorization.kubernetes.io/autoupdate: "true"  # 该注解表示此角色会自动更新
#   labels:
#     kubernetes.io/bootstrapping: rbac-defaults  # 标记该角色为 Kubernetes 引导过程中使用的默认角色
#   name: system:kube-apiserver-to-kubelet  # 角色名称，指明该角色允许 API Server 与 kubelet 之间的通信
# rules:
#   - apiGroups:
#       - ""  # 适用于核心 Kubernetes 资源 (无 API Group)
#     resources:
#       - nodes/proxy   # 允许访问节点代理 API
#       - nodes/stats   # 允许访问节点的统计信息
#       - nodes/log     # 允许访问节点日志
#       - nodes/spec    # 允许访问节点规范
#       - nodes/metrics # 允许访问节点度量信息
#     verbs:
#       - "*"  # 该角色具有对上述资源的所有操作权限
# ---
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   name: system:kube-apiserver    # ClusterRoleBinding 的名称是 system:kube-apiserver
#   namespace: ""                  # ClusterRoleBinding 适用于所有命名空间
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: system:kube-apiserver-to-kubelet  # 引用前面定义的 ClusterRole，授予 kube-apiserver 与 kubelet 之间的权限
# subjects:
#   - apiGroup: rbac.authorization.k8s.io
#     kind: User
#     name: kube-apiserver  # 将该权限授予 kube-apiserver 用户
```

##### 11.3.2 验证yaml文件语法是否正确

```bash
[root@master01 ssl]# kubectl apply -f bootstrap-secret.yaml --dry-run=client
secret/bootstrap-token-10001 created (dry run)
clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created (dry run)
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created (dry run)
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created (dry run)
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet configured (dry run)
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created (dry run)


#secret/bootstrap-token-10001 created (dry run)：表示 Secret 被成功创建，但这只是一个 Dry Run，实际的创建操作还未发生。
#clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created (dry run)：表示 ClusterRoleBinding 已成功验证，属于 Dry Run。
#其他的 ClusterRoleBinding 和 ClusterRole 也都显示成功，并且是 Dry Run 状态。
#如果一切配置无误，下一步可以实际应用配置：如果你确认这些配置是正确的，并且没有遗漏任何必要的部分，可以去掉 --dry-run=client 选项并实际执行应用命令来创建资源。
```

##### 11.3.3 应用bootstrap-secret配置文件

```bash
[root@master01 ~]# kubectl apply -f bootstrap-secret.yaml
secret/bootstrap-token created
clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created

# 查看验证
[root@master01 ~]#  
NAME                     TYPE                            DATA   AGE
bootstrap-token-10001   bootstrap.kubernetes.io/token   6      54m
[root@master01 ~]# kubectl get clusterrolebinding kubelet-bootstrap
NAME                ROLE                                   AGE
kubelet-bootstrap   ClusterRole/system:node-bootstrapper   3d5h
[root@master01 ~]# kubectl get clusterrolebinding node-autoapprove-bootstrap
NAME                         ROLE                                                                           AGE
node-autoapprove-bootstrap   ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient   3d5h
[root@master01 ~]# kubectl get clusterrolebinding node-autoapprove-certificate-rotation
NAME                                    ROLE                                                                               AGE
node-autoapprove-certificate-rotation   ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   3d5h
[root@master01 ~]# kubectl get clusterrolebinding system:kube-apiserver
NAME                    ROLE                                           AGE
system:kube-apiserver   ClusterRole/system:kube-apiserver-to-kubelet   3d5h
```



### 12.部署worker节点

#### 12.1）从master01复制证书到其他节点

```bash
[root@master01 pki]# for i in  node0{1,2,3};do ssh $i "mkdir -pv /etc/kubernetes/ssl/";done

[root@master01 ~]# for i in master0{2,3} node0{1,2,3};do echo $i; scp /etc/kubernetes/bootstrap-kubelet.kubeconfig $i:/etc/kubernetes/ ; done
master02
bootstrap-kubelet.kubeconfig                   100% 2210     1.3MB/s   00:00
master03
bootstrap-kubelet.kubeconfig                   100% 2210     1.6MB/s   00:00    
node01
bootstrap-kubelet.kubeconfig                   100% 2210     1.4MB/s   00:00    
node02
bootstrap-kubelet.kubeconfig                   100% 2210     1.5MB/s   00:00    
node03
bootstrap-kubelet.kubeconfig                   100% 2210     1.4MB/s   00:00

[root@master01 ~]# for i in node0{1,2,3};do cd /etc/kubernetes/ssl ;echo $i;scp k8s-ca.pem k8s-ca-key.pem front-proxy-ca.pem $i:/etc/kubernetes/ssl/;done
node01
k8s-ca.pem                                     100% 1363   871.8KB/s   00:00    
k8s-ca-key.pem                                 100% 1675     1.8MB/s   00:00    
front-proxy-ca.pem                             100% 1094     1.0MB/s   00:00    
node02
k8s-ca.pem                                     100% 1363   772.6KB/s   00:00    
k8s-ca-key.pem                                 100% 1675     2.0MB/s   00:00    
front-proxy-ca.pem                             100% 1094     1.5MB/s   00:00    
node03
k8s-ca.pem                                     100% 1363   750.3KB/s   00:00    
k8s-ca-key.pem                                 100% 1675     1.1MB/s   00:00    
front-proxy-ca.pem                             100% 1094   688.1KB/s   00:00 
```

#### 12.2）启动kubelet服务

##### 12.2.1 所有节点创建工作目录

```bash
mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests/
```

##### 12.2.2 所有节点创建kubelet的配置文件

```bash
#设置与 Kubernetes 集群其他组件协作、如何管理资源、如何控制容器运行时的行为等方面
cat > /etc/kubernetes/kubelet-conf.yml <<'EOF'
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/ssl/k8s-ca.pem
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
cgroupDriver: systemd
cgroupsPerQOS: true
clusterDNS:
  - 10.96.0.10
clusterDomain: cluster.local
containerLogMaxFiles: 5
containerLogMaxSize: 10Mi
contentType: application/vnd.kubernetes.protobuf
cpuCFSQuota: true
cpuManagerPolicy: none
cpuManagerReconcilePeriod: 10s
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
  - pods
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionPressureTransitionPeriod: 5m0s
failSwapOn: true
fileCheckFrequency: 20s
hairpinMode: promiscuous-bridge
healthzBindAddress: 0.0.0.0
healthzPort: 10248
httpCheckFrequency: 20s
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
imageMinimumGCAge: 2m0s
iptablesDropBit: 15
iptablesMasqueradeBit: 14
kubeAPIBurst: 10
kubeAPIQPS: 5
makeIPTablesUtilChains: true
maxOpenFiles: 1000000
maxPods: 110
nodeStatusUpdateFrequency: 10s
oomScoreAdj: -999
podPidsLimit: -1
registryBurst: 10
registryPullQPS: 5
resolvConf: /etc/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 2m0s
serializeImagePulls: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 4h0m0s
syncFrequency: 1m0s
volumeStatsAggPeriod: 1m0s
EOF
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
apiVersion: kubelet.config.k8s.io/v1beta1  # 指定 kubelet 配置文件的版本。此版本用于配置 kubelet 的行为和属性。v1beta1 表示这是 kubelet 配置的一个较新的版本。

kind: KubeletConfiguration  # 指定该文件的类型为 KubeletConfiguration，表明这个配置文件是针对 kubelet 服务的配置，kubelet 是 Kubernetes 节点上的代理进程，负责节点的状态管理和容器运行。
address: 0.0.0.0  # 设置 kubelet 监听的 IP 地址。0.0.0.0 表示 kubelet 会监听所有可用的网络接口地址。
port: 10250  # 指定 kubelet 监听的端口，10250 是 kubelet 默认的端口，用于与其他组件（如 kube-apiserver）进行通信。
readOnlyPort: 10255  # 指定 kubelet 提供只读端口（10255），允许通过 HTTP 协议查看 kubelet 状态（例如节点信息、容器信息等）。此端口不需要认证，因此要谨慎使用。
authentication:  # 设置认证相关参数，用于控制 API 请求的身份验证方式。
  anonymous:
    enabled: false  # 禁用匿名访问，要求所有请求都需要进行身份验证。
  webhook:
    cacheTTL: 2m0s  # 设置 Webhook 认证缓存的存活时间为 2 分钟，避免频繁地进行 Webhook 认证。
    enabled: true  # 启用 Webhook 认证，kubelet 将通过 Webhook 调用外部服务来验证请求身份。
  x509:
    clientCAFile: /certs/kubernetes/k8s-ca.pem  # 指定客户端证书的 CA 文件路径，用于验证客户端请求的证书是否有效。
authorization:  # 设置授权相关参数，用于控制是否允许某些用户或服务执行特定操作。
  mode: Webhook  # 指定授权模式为 Webhook，kubelet 将通过 Webhook 请求外部授权服务来判断是否允许执行某个操作。
  webhook:
    cacheAuthorizedTTL: 5m0s  # 授权的缓存时间为 5 分钟。已授权的请求会被缓存，避免每次都需要重新验证。
    cacheUnauthorizedTTL: 30s  # 未授权的请求会被缓存 30 秒，减少频繁的授权检查。
cgroupDriver: systemd  # 指定 kubelet 使用 systemd 作为 cgroup 驱动。systemd 驱动通常与现代 Linux 系统兼容，提供更好的资源隔离和控制。
cgroupsPerQOS: true  # 启用每个质量保证（QOS）级别的 cgroup 支持。kubelet 会根据 Pod 的 QOS 类别（Guaranteed, Burstable, BestEffort）分别设置不同的 cgroup 策略。
clusterDNS:
  - 10.96.0.10  # 指定 Kubernetes 集群的 DNS 服务 IP 地址，这里是 10.96.0.10。kubelet 将使用这个 DNS 地址解析集群内的服务名称。
clusterDomain: cluster.local  # 指定 Kubernetes 集群的域名后缀，所有服务名称的 DNS 查询会以此后缀作为默认域名（例如 myservice.cluster.local）。
containerLogMaxFiles: 5  # 指定容器日志文件的最大数量。kubelet 会保留最多 5 个日志文件，当日志文件数量超过这个值时，最旧的日志会被删除。
containerLogMaxSize: 10Mi  # 指定每个容器日志文件的最大大小。kubelet 会在日志文件达到 10Mi 大小时进行切割。
contentType: application/vnd.kubernetes.protobuf  # 指定 kubelet 使用的内容类型为 protobuf，这是 Kubernetes API 通信的默认序列化格式，性能更高，适合大规模集群。
cpuCFSQuota: true  # 启用 CPU CFS（Completely Fair Scheduler）配额限制。kubelet 会使用此机制来控制容器的 CPU 使用率，以确保容器不会超出其分配的 CPU 资源。
cpuManagerPolicy: none  # 指定 kubelet 的 CPU 管理策略。none 表示不启用任何 CPU 管理策略。如果启用了其他策略（如 static），kubelet 会将 Pod 分配到特定的 CPU 核心。
cpuManagerReconcilePeriod: 10s  # 指定 kubelet CPU 管理策略的重新调节周期为 10 秒。kubelet 将每 10 秒重新评估是否需要调整容器的 CPU 资源分配。
enableControllerAttachDetach: true  # 启用控制器附加/分离功能。此功能使得 Kubernetes 控制器可以自动将卷挂载到 Pod 中并在需要时分离。
enableDebuggingHandlers: true  # 启用调试处理程序，允许通过 HTTP 请求访问调试信息，帮助诊断 kubelet 的问题。
enforceNodeAllocatable:
  - pods  # 强制节点分配策略，确保一定数量的资源（如 CPU、内存）保留给 Pods 使用，不允许其他系统进程占用。
eventBurst: 10  # 设置事件的最大突发数量为 10。即在单位时间内，kubelet 能够处理的最大事件数。
eventRecordQPS: 5  # 设置事件记录的速率为每秒 5 次，防止事件过多时对系统造成压力。
evictionHard:  # 设置硬性驱逐条件，当以下资源低于指定阈值时，kubelet 将驱逐 Pod 来释放资源：
  imagefs.available: 15%  # 当镜像文件系统的可用空间低于 15% 时，驱逐 Pod。
  memory.available: 100Mi  # 当可用内存低于 100Mi 时，驱逐 Pod。
  nodefs.available: 10%  # 当节点文件系统的可用空间低于 10% 时，驱逐 Pod。
  nodefs.inodesFree: 5%  # 当节点文件系统的空闲 inode 数量低于 5% 时，驱逐 Pod。
evictionPressureTransitionPeriod: 5m0s  # 指定驱逐压力的过渡期为 5 分钟。即在这个时间窗口内，如果资源压力条件被满足，kubelet 会开始驱逐 Pod。
failSwapOn: true  # 启用交换分区检查。如果启用，当系统启用 swap（交换空间）时，kubelet 将拒绝启动节点。
fileCheckFrequency: 20s  # 设置文件检查频率为 20 秒。kubelet 会每 20 秒检查容器文件系统的状态。
hairpinMode: promiscuous-bridge  # 启用 promiscuous-bridge 模式，允许容器网络接口接收来自所有源的流量，这有助于支持某些网络模式和应用。
healthzBindAddress: 0.0.0.0  # 设置健康检查接口的绑定地址为 0.0.0.0。
healthzPort: 10248  # 设置健康检查的端口为 10248，kubelet 会在该端口提供健康检查服务。
httpCheckFrequency: 20s  # 设置 HTTP 检查频率为 20 秒，kubelet 会每 20 秒进行一次 HTTP 请求健康检查。
imageGCHighThresholdPercent: 85  # 设置镜像垃圾回收的高阈值为 85%，即当节点的镜像磁盘使用超过 85% 时，会触发镜像回收。
imageGCLowThresholdPercent: 80  # 设置镜像垃圾回收的低阈值为 80%，即当节点的镜像磁盘使用低于 80% 时，回收会停止。
imageMinimumGCAge: 2m0s  # 设置镜像回收的最小年龄为 2 分钟。即只有 2 分钟之前未使用的镜像才会被回收。
iptablesDropBit: 15  # 指定 iptables 设置的丢包比特位，通常用于控制网络流量。
iptablesMasqueradeBit: 14  # 指定 iptables 中伪装的比特位。这个参数用于网络地址转换（NAT）规则，允许在网络包流动时进行 IP 地址伪装，使得多个容器可以共享同一个外部 IP 地址。
kubeAPIBurst: 10  # 设置 kubelet 连接到 Kubernetes API 服务器时的请求突发量为 10。这意味着 kubelet 在短时间内最多可以发送 10 个请求，如果超过这个突发量，后续的请求会被限制。
kubeAPIQPS: 5  # 指定 kubelet 向 Kubernetes API 服务器发送请求的最大 QPS（每秒请求数）。设置为 5 意味着 kubelet 每秒最多只能发起 5 次 API 请求。
makeIPTablesUtilChains: true  # 启用 iptables 工具链的创建。kubelet 会自动创建一些必需
makeIPTablesUtilChains: true  # 启用 iptables 工具链的创建。kubelet 会自动创建一些必需的 iptables 规则来管理容器网络流量。
maxOpenFiles: 1000000  # 设置 kubelet 可以打开的最大文件数为 1,000,000。该配置项控制 kubelet 进程能够打开的最大文件描述符数量，适用于大规模集群或容器环境，防止文件句柄耗尽。
maxPods: 110  # 设置每个节点可以运行的最大 Pod 数量为 110。这个值决定了 kubelet 在节点上可以调度的最大 Pod 数量。如果节点上的 Pod 数量超过该值，kubelet 将无法调度更多的 Pod。
nodeStatusUpdateFrequency: 10s  # 设置节点状态更新的频率为每 10 秒。kubelet 会定期将节点状态（如容量、资源使用等）报告给 Kubernetes API 服务器，以确保集群中的节点信息是最新的。
oomScoreAdj: -999  # 设置 OOM（Out Of Memory）得分调整值为 -999。这个值用于调整操作系统的 OOM 选择策略，值越低，kubelet 进程在系统内存不足时越不容易被杀死。
podPidsLimit: -1  # 设置 Pod 中的进程数限制为 -1，表示没有限制。该配置项用于控制 Pod 中可以运行的最大进程数，默认值 -1 表示没有限制。
registryBurst: 10  # 指定 kubelet 向容器镜像仓库发起请求时的突发量为 10。即在短时间内，kubelet 最多可以发起 10 次镜像拉取请求。
registryPullQPS: 5  # 设置每秒钟 kubelet 从容器镜像仓库拉取镜像的请求数为 5。这有助于限制 kubelet 的镜像拉取速率，防止过多的请求对镜像仓库造成压力。
resolvConf: /etc/resolv.conf  # 指定 DNS 配置文件的位置。kubelet 将使用此文件来配置 Pod 内的 DNS 解析。
rotateCertificates: true  # 启用证书轮换。当启用时，kubelet 将定期更新其证书，以确保安全性，并避免证书过期。
runtimeRequestTimeout: 2m0s  # 指定容器运行时请求的超时时间为 2 分钟。如果 kubelet 在 2 分钟内未收到运行时（如 Docker 或 containerd）的响应，将视为超时。
serializeImagePulls: true  # 启用镜像拉取的串行化。在同一节点上，多个 Pod 的镜像拉取操作将按顺序进行，而不是并行进行。这样做可以减少并发拉取镜像时对网络和存储的压力。
staticPodPath: /etc/kubernetes/manifests  # 指定静态 Pod 配置文件的路径。kubelet 会扫描此路径中的文件，并将这些静态 Pod 作为容器进行启动和管理。
streamingConnectionIdleTimeout: 4h0m0s  # 设置流连接空闲超时时间为 4 小时。表示在 4 小时内如果连接处于空闲状态（没有数据交换），连接将会被关闭。用于控制长连接的空闲超时，避免占用资源。
syncFrequency: 1m0s  # 设置 kubelet 进行同步操作的频率为 1 分钟。同步操作涉及到从 kube-apiserver 获取新的资源信息、状态更新等，这有助于保持 kubelet 和集群状态的同步。
volumeStatsAggPeriod: 1m0s  # 设置卷统计的聚合周期为 1 分钟。kubelet 会每 1 分钟收集一次卷的统计数据，并将其发送给集群监控系统。
------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
# 以下是可能还存在的其他参数：
nodeLabels:  # 用于为节点添加标签
  kubernetes.io/role: worker  # 添加节点标签，指定角色为 worker 节点
maxPerPodContainerCount: 50  # 设置每个 Pod 最多允许有 50 个容器。如果 Pod 中的容器数超过此限制，kubelet 将拒绝调度该 Pod。
imagePullProgressDeadline: 1m0s  # 设置镜像拉取进度的超时期限为 1 分钟。如果镜像拉取在此时间内未完成，kubelet 会认为该拉取操作失败。
shutdownGracePeriod: 30s  # 设置节点关闭时的优雅关闭期限为 30 秒。在此时间内，Pod 会尝试优雅终止。
shutdownGracePeriodCriticalPods: 10s  # 设置关键 Pod（如 kube-system 中的 Pod）在节点关闭时的优雅关闭期限为 10 秒。
failSwapOn: true  # 设置是否在启用 swap 时拒绝启动节点。如果启用，当系统启用 swap（交换空间）时，kubelet 将拒绝启动该节点。
evictionSoft:
  memory.available: 1Gi  # 当节点的内存低于 1 Gi 时，kubelet 会根据软驱逐条件进行资源回收。即它会开始驱逐 Pod，但不会立即执行。
  imagefs.available: 15%  # 当镜像文件系统的可用空间低于 15% 时，kubelet 会根据软驱逐条件进行资源回收。
evictionSoftGracePeriod:
  memory.available: 1m0s  # 在满足软驱逐条件后，kubelet 将等待 1 分钟再执行驱逐操作，以提供更多的容错时间。
  imagefs.available: 30s  # 在满足软驱逐条件后，kubelet 将等待 30 秒再执行驱逐操作。
evictionApiVersion: v1  # 设置驱逐操作的 API 版本为 v1。控制 kubelet 使用的驱逐 API 版本。
evictionMaxPodGracePeriod: 120s  # 设置驱逐 Pod 时允许的最大优雅终止时间为 120 秒。如果 Pod 在此时间内未正常终止，kubelet 会强制终止它。
volumePluginDir: /etc/kubernetes/kubelet-plugins/volume/exec  # 指定 kubelet 插件的目录。kubelet 会扫描该目录中的所有插件来支持外部存储卷。
# 以下是与安全相关的设置：
anonymousAuth: false  # 禁止匿名访问 kubelet。所有访问 kubelet 的请求必须经过身份验证。
tlsCipherSuites:  # 配置 kubelet 支持的 TLS 加密套件列表。用于强化网络通信的安全性。
  - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
  - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
```

##### 12.2.3 所有节点创建service启动文件

```bash
#用于 systemd 启动和管理 kubelet 服务的，包含了服务的描述和启动细节：
cat >  /usr/lib/systemd/system/kubelet.service <<'EOF'
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet
Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
# [Unit] 部分:

# Description=Kubernetes Kubelet：这只是一个描述字符串，标明该服务是 "Kubernetes Kubelet"。
# Documentation=https://github.com/kubernetes/kubernetes：提供该服务的文档链接，可以帮助用户了解 Kubernetes 和 kubelet 的详细信息。
# After=containerd.service：指定 kubelet 服务在启动时需要等待 containerd 容器运行时服务启动完成后再启动。
# Requires=containerd.service：表示 kubelet 服务依赖于 containerd 服务。如果 containerd 服务没有成功启动，kubelet 服务也会无法启动。
# [Service] 部分:

# ExecStart=/usr/local/bin/kubelet：指定 kubelet 服务的启动命令路径，指向 /usr/local/bin/kubelet，即 kubelet 可执行文件的位置。
# Restart=always：设置服务始终自动重启。当 kubelet 服务意外停止时，systemd 会自动重启该服务。
# StartLimitInterval=0：设置服务的重启限制间隔为 0，表示不限制服务启动的频率。即使 kubelet 启动失败，systemd 也会尝试重新启动它。
# RestartSec=10：指定服务重启时的等待时间为 10 秒，避免频繁重启导致的资源浪费。
# [Install] 部分:

# WantedBy=multi-user.target：表示该服务在系统启动时需要被启用，在 multi-user.target 运行级别下启动。这通常意味着服务将在多用户模式下启动，适用于大多数非图形化服务器环境。
```

```bash
#为 kubelet 服务提供额外的环境变量和配置。它的作用是修改或扩展 kubelet 的启动参数。这个文件不需要重新启动整个服务，只会影响 kubelet 服务的启动方式
cat > /etc/systemd/system/kubelet.service.d/10-kubelet.conf <<'EOF'
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig"
Environment="KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml"
Environment="KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock"
Environment="KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node='' "
ExecStart=
ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS
EOF

------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/certs/kubeconfig/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig"：# 该环境变量配置了 kubelet 的两个 kubeconfig 文件路径。一个用于引导过程（bootstrap-kubeconfig），另一个是主要的 kubelet 配置文件。"kubelet.kubeconfig"文件并不存在，这个证书文件后期会自动生成;
Environment="KUBELET_CONFIG_ARGS=--config=/etc/kubernetes/kubelet-conf.yml"：# 该环境变量指定了 kubelet 的主配置文件的位置。kubelet-conf.yml 文件通常包含节点级别的设置，例如网络插件、资源限制等。
Environment="KUBELET_SYSTEM_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock"：# 该环境变量指定容器运行时的 API 端点，指向正在运行的 containerd 容器运行时服务。containerd 是一种常用的容器运行时，可以与 kubelet 配合使用。
Environment="KUBELET_EXTRA_ARGS=--node-labels=node.kubernetes.io/node='' "：# 这个环境变量设置了额外的启动参数，为 kubelet 节点添加标签。在这个例子中，给 kubelet 节点添加了一个空的标签 node.kubernetes.io/node=''，可能是为了标记某些特定的节点。
ExecStart=:# 这个指令会覆盖之前在 kubelet.service 文件中定义的 ExecStart 行。它将使用通过环境变量传递的配置参数重新构造 ExecStart 命令行。
ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_SYSTEM_ARGS $KUBELET_EXTRA_ARGS：# 该行表示 kubelet 服务的启动命令将通过结合不同的环境变量来执行。这确保了所有指定的配置都被传递给 kubelet 启动过程。
```

##### 12.2.4 启动所有节点kubelet

```bash
systemctl daemon-reload && systemctl enable --now kubelet
[root@master01 ssl]# systemctl status kubelet
● kubelet.service - Kubernetes Kubelet
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubelet.conf
   Active: active (running) since Tue 2024-11-19 09:38:03 CST; 1s ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 8103 (kubelet)
    Tasks: 5
   Memory: 117.8M
   CGroup: /system.slice/kubelet.service
           └─8103 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --config=/etc/kubernetes/kubelet-conf.yml --container-runtime-endpoint=unix:///run/containerd/containerd.sock --node-labels=node.kubernetes.io/node=

Nov 19 09:38:03 master01 systemd[1]: Started Kubernetes Kubelet.
Nov 19 09:38:03 master01 kubelet[8103]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Nov 19 09:38:04 master01 kubelet[8103]: I1119 09:38:04.429361    8103 server.go:467] "Kubelet version" kubeletVersion="v1.28.15"
Nov 19 09:38:04 master01 kubelet[8103]: I1119 09:38:04.429496    8103 server.go:469] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Nov 19 09:38:04 master01 kubelet[8103]: I1119 09:38:04.429820    8103 server.go:895] "Client rotation is on, will bootstrap in background"
Nov 19 09:38:04 master01 kubelet[8103]: I1119 09:38:04.442798    8103 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/ssl/k8s-ca.pem"
```

##### 12.2.5 查看节点信息

```bash
[root@master01 ~]# kubectl get node
NAME       STATUS     ROLES    AGE   VERSION
master01   NotReady   <none>   9h    v1.28.15
master02   NotReady   <none>   38s   v1.28.15
master03   NotReady   <none>   50s   v1.28.15
node01     NotReady   <none>   41s   v1.28.15
node02     NotReady   <none>   40s   v1.28.15
node03     NotReady   <none>   40s   v1.28.15
[root@master01 ~]# kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-4r9zw   55s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:c8ad9c   <none>              Approved,Issued
csr-76q6s   52s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:c8ad9c   <none>              Approved,Issued
csr-7v6m9   55s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:c8ad9c   <none>              Approved,Issued
csr-svmxq   55s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:c8ad9c   <none>              Approved,Issued
csr-vcl6l   64s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:c8ad9c   <none>              Approved,Issued
```



#### 12.3）启动kube-proxy服务

##### 12.3.1 创建kube-proxy需要的证书文件

```bash
cd /etc/kubernetes/pki


#生成kube-proxy的csr文件
cat > /etc/kubernetes/pki/kube-proxy-csr.json << EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:kube-proxy",
      "OU": "Kubernetes-manual"
    }
  ]
}
EOF


#创建kube-proxy需要的证书文件
cfssl gencert \
  -ca=/etc/kubernetes/ssl/k8s-ca.pem \
  -ca-key=/etc/kubernetes/ssl/k8s-ca-key.pem \
  -config=/etc/kubernetes/pki/k8s-ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare /etc/kubernetes/ssl/kube-proxy
  
  
[root@master01 pki]# ll /etc/kubernetes/ssl/kube-proxy*
-rw-r--r-- 1 root root 1045 Nov 21 09:46 /etc/kubernetes/ssl/kube-proxy.csr
-rw------- 1 root root 1679 Nov 21 09:46 /etc/kubernetes/ssl/kube-proxy-key.pem
-rw-r--r-- 1 root root 1464 Nov 21 09:46 /etc/kubernetes/ssl/kube-proxy.pem  
```

##### 12.3.2 生成proxy的.kubeconfig文件发到所有节点

```bash
#1. 设置集群
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/k8s-ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.40:8443 \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

#2. 设置一个用户项
kubectl config set-credentials system:kube-proxy \
  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

#3. 设置一个上下文环境
kubectl config set-context kube-proxy@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-proxy \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

#4. 使用默认的上下文
 kubectl config use-context kube-proxy@kubernetes \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

#5. 将kube-proxy的systemd Service文件发送到其他节点
[root@master01 pki]# for i in master0{2,3} node0{1,2,3} ; do echo $i ;scp /etc/kubernetes/kube-proxy.kubeconfig $i:/etc/kubernetes/ ;done
master02
kube-proxy.kubeconfig                                                           100% 6402     2.5MB/s   00:00    
master03
kube-proxy.kubeconfig                                                           100% 6402     3.1MB/s   00:00    
node01
kube-proxy.kubeconfig                                                           100% 6402     2.5MB/s   00:00    
node02
kube-proxy.kubeconfig                                                           100% 6402     2.1MB/s   00:00    
node03
kube-proxy.kubeconfig                                                           100% 6402     1.5MB/s   00:00 
```

##### 12.3.3 所有节点创建kube-proxy.conf配置文件

```bash
cat > /etc/kubernetes/kube-proxy.yml << EOF
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 0.0.0.0
metricsBindAddress: 127.0.0.1:10249
clientConnection:
  acceptConnection: ""
  burst: 10
  contentType: application/vnd.kubernetes.protobuf
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
  qps: 5
clusterCIDR: 172.16.0.0/16
configSyncPeriod: 15m0s
conntrack:
  max: null
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
enableProfiling: false
healthzBindAddress: 0.0.0.0:10256
hostnameOverride: ""
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 0s
ipvs:
  masqueradeAll: true
  minSyncPeriod: 5s
  scheduler: "rr"
  syncPeriod: 30s
mode: "ipvs"
nodeProtAddress: null
oomScoreAdj: -999
portRange: ""
udpIdelTimeout: 250ms
EOF

------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------
apiVersion: kubeproxy.config.k8s.io/v1alpha1          : 配置文件的 API 版本，表示这是 kube-proxy 配置的版本。
kind: KubeProxyConfiguration                         : 配置文件的类型，表示这是一个 KubeProxy 配置文件。
bindAddress: 0.0.0.0                                : 设置 kube-proxy 绑定的 IP 地址，0.0.0.0 表示监听所有网络接口。
metricsBindAddress: 127.0.0.1:10249                  : 设置 kube-proxy 的指标服务器地址和端口，这里设置为本地的 127.0.0.1:10249。
clientConnection:                                   : 配置客户端与 kube-apiserver 连接的参数。
  acceptConnection: ""                              : 是否接受新的连接（为空表示默认行为）。
  burst: 10                                         : 用于控制客户端连接的流量突发，表示允许的最大突发请求数。
  contentType: application/vnd.kubernetes.protobuf  : 设置客户端与 API server 通信时的内容类型，这里使用 protobuf 格式。
  kubeconfig: /yinzhengjie/certs/kubeconfig/kube-proxy.kubeconfig : 指定 kube-proxy 使用的 kubeconfig 文件路径，用于与 Kubernetes API server 进行通信。
  qps: 5                                            : 设置每秒请求数（QPS），用于限制客户端与 API server 之间的请求频率。
clusterCIDR: 172.16.0.0/16                          : 设置 Kubernetes 集群的 CIDR 范围，这里表示集群中的 Pod 地址范围。
configSyncPeriod: 15m0s                             : 配置 kube-proxy 同步配置的时间间隔，表示每 15 分钟重新加载配置。
conntrack:                                          : 设置连接跟踪（conntrack）相关的参数。
  max: null                                          : 最大连接数，null 表示使用系统默认值。
  maxPerCore: 32768                                 : 每个 CPU 核心的最大连接数限制。
  min: 131072                                        : 最小连接数，表示 conntrack 维护的最小连接数。
  tcpCloseWaitTimeout: 1h0m0s                        : TCP 连接处于 CLOSE_WAIT 状态时的超时时间，默认为 1 小时。
  tcpEstablishedTimeout: 24h0m0s                    : TCP 连接处于 ESTABLISHED 状态时的超时时间，默认为 24 小时。
enableProfiling: false                              : 是否启用 profiling（性能分析），设置为 `false` 禁用，`true` 启用。
healthzBindAddress: 0.0.0.0:10256                   : 设置健康检查的地址和端口，这里设置为所有接口的 10256 端口。
hostnameOverride: ""                                 : 用于设置 kube-proxy 的主机名，通常留空表示自动使用主机名。
iptables:                                           : 配置 iptables 相关的参数。
  masqueradeAll: false                              : 是否开启所有流量的 SNAT（源地址伪装），设置为 `false` 表示不对流量进行 SNAT。
  masqueradeBit: 14                                 : 在使用 iptables 时，设置用于进行源地址伪装的位，默认为 14。
  minSyncPeriod: 0s                                  : 设置最小同步周期，默认为 0 秒，表示立刻同步。
ipvs:                                               : 配置 IPVS（IP Virtual Server）相关的参数。
  masqueradeAll: true                               : 启用 IPVS 的源地址伪装（SNAT）。
  minSyncPeriod: 5s                                 : 设置 IPVS 同步的最小时间间隔，这里为 5 秒。
  scheduler: "rr"                                   : 配置负载均衡算法，"rr" 表示轮询（round robin）调度算法。
  syncPeriod: 30s                                   : 配置 IPVS 同步的时间间隔，默认为 30 秒。
mode: "ipvs"                                        : 设置 kube-proxy 的代理模式，这里使用 IPVS 模式。
nodeProtAddress: null                               : 设置节点保护的地址，默认为 `null`，即不设置。
oomScoreAdj: -999                                   : 设置 OOM（Out Of Memory）时，kube-proxy 的进程优先级，-999 表示 kube-proxy 将不会被 OOM 杀死。
portRange: ""                                       : 配置 kube-proxy 使用的端口范围，留空表示不限制。
udpIdelTimeout: 250ms                               : 配置 UDP 连接的空闲超时时间，表示如果 UDP 连接在 250 毫秒内没有数据传输则关闭连接。
```

##### 12.3.4 所有节点使用systemd 管理 kube-proxy

```bash
cat > /usr/lib/systemd/system/kube-proxy.service << EOF
[Unit]
Description= Kubernetes Proxy
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yml \
  --v=2 
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF


#所有节点启动kube-proxy
systemctl daemon-reload && systemctl enable --now kube-proxy
systemctl status kube-proxy
```

### 13.安装Calico、Core DNS、Metrics Server

#### 13.1）Calico安装

**下载 calico的  yaml文件**

```bash
#前往gitee仓库下载 calio文件
https://gitee.com/dukuan/k8s-ha-install/tree/manual-installation-v1.28.x/

#上传到服务器中
```

**启动 calico**

```bash
#更改calico的网段，改为自己的Pod网段,如果这里修改错误 或者 API证书那里没有添加 svc的第一个地址，这里是有可能会下载失败的，下载的calico 注意版本和k8s 要兼容
sed -i "s#POD_CIDR#172.16.0.0/16#g" calico.yaml

# 下载安装calilo
kubectl apply -f calico.yaml

[root@master01 kubernetes]# kubectl get po -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6d48795585-795b4   1/1     Running   0          3m6s
calico-node-7xxv5                          1/1     Running   0          3m6s
calico-node-b2pwk                          1/1     Running   0          3m6s
calico-node-hp6rt                          1/1     Running   0          3m6s
calico-node-n5s7h                          1/1     Running   0          3m6s
calico-node-pjpcw                          1/1     Running   0          3m6s
calico-node-wjbhh                          1/1     Running   0          3m6s
```

#### 13.2）Core DNS 安装

**下载Core DNS的  yaml文件**

```bash
#前往gitee仓库下载 Core DNS文件
https://gitee.com/dukuan/k8s-ha-install/tree/manual-installation-v1.28.x/
```

**启动 Core DNS**

```bash
#将coredns的serviceIP改成k8s service网段的第十个IP
[root@master01 kubernetes]# kubectl get svc | grep kubernetes
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d10h

COREDNS_SERVICE_IP=`kubectl get svc | grep kubernetes | awk '{print $3}'`0
sed -i "s#KUBEDNS_SERVICE_IP#${COREDNS_SERVICE_IP}#g" coredns.yaml

[root@master01 kubernetes]# kubectl apply -f coredns.yaml 
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created


[root@master01 kubernetes]# kubectl get pods -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6d48795585-795b4   1/1     Running   0          15m
kube-system   calico-node-7xxv5                          1/1     Running   0          15m
kube-system   calico-node-b2pwk                          1/1     Running   0          15m
kube-system   calico-node-hp6rt                          1/1     Running   0          15m
kube-system   calico-node-n5s7h                          1/1     Running   0          15m
kube-system   calico-node-pjpcw                          1/1     Running   0          15m
kube-system   calico-node-wjbhh                          1/1     Running   0          15m
kube-system   coredns-788958459b-x57st                   1/1     Running   0          2m17s
```

#### 13.3）Metrics Server 安装

<span style="background-color: yellow;">在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率</span>

**下载Metrics Server的  yaml文件**

```bash
#前往gitee仓库下载 Metrics Server文件
https://gitee.com/dukuan/k8s-ha-install/tree/manual-installation-v1.28.x/
```

**启动 Metrics Server**

```bash
#注意将yaml文件中的证书路径进行修改为实际的证书路径
--requestheader-client-ca-file=
- mountPath:
path:

[root@master01 kubernetes]# kubectl apply -f comp.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created



[root@master01 kubernetes]# kubectl get pod -A
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-6d48795585-795b4   1/1     Running   0          29m
kube-system   calico-node-7xxv5                          1/1     Running   0          29m
kube-system   calico-node-b2pwk                          1/1     Running   0          29m
kube-system   calico-node-hp6rt                          1/1     Running   0          29m
kube-system   calico-node-n5s7h                          1/1     Running   0          29m
kube-system   calico-node-pjpcw                          1/1     Running   0          29m
kube-system   calico-node-wjbhh                          1/1     Running   0          29m
kube-system   coredns-788958459b-x57st                   1/1     Running   0          16m
kube-system   metrics-server-9989448db-bhf6k             1/1     Running   0          2m38s
```

**查看集群资源**

```bash
[root@master01 kubernetes]# kubectl top nodes
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master01   301m         30%    1712Mi          44%       
master02   225m         22%    1057Mi          56%       
master03   222m         22%    1155Mi          61%       
node01     204m         10%    914Mi           23%       
node02     108m         10%    480Mi           55%       
node03     112m         11%    451Mi           52%  
```





### 14.集群验证

**安装 busybox pod**

```bash
cat<<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
EOF

```

1.验证节点是否正常

```bash
[root@master01 ~]# kubectl get node -o wide
NAME       STATUS   ROLES    AGE     VERSION    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
master01   Ready    <none>   4d15h   v1.28.15   192.168.0.50   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
master02   Ready    <none>   4d6h    v1.28.15   192.168.0.51   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
master03   Ready    <none>   4d6h    v1.28.15   192.168.0.52   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
node01     Ready    <none>   4d6h    v1.28.15   192.168.0.60   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
node02     Ready    <none>   4d6h    v1.28.15   192.168.0.61   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
node03     Ready    <none>   4d6h    v1.28.15   192.168.0.62   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33

#确认 STATUS 是否为 Ready
```

2.验证pod是否正常

```bash
[root@master01 ~]# kubectl get pod -A -o wide
NAMESPACE     NAME                                       READY   STATUS    RESTARTS        AGE    IP               NODE       NOMINATED NODE   READINESS GATES
default       busybox                                    1/1     Running   6 (46m ago)     3d1h   172.16.241.67    master01   <none>           <none>
kube-system   calico-kube-controllers-6d48795585-795b4   1/1     Running   1 (4h27m ago)   3d6h   172.16.196.136   node01     <none>           <none>
kube-system   calico-node-7xxv5                          1/1     Running   1 (4h27m ago)   3d6h   192.168.0.50     master01   <none>           <none>
kube-system   calico-node-b2pwk                          1/1     Running   1 (4h27m ago)   3d6h   192.168.0.60     node01     <none>           <none>
kube-system   calico-node-hp6rt                          1/1     Running   1 (4h27m ago)   3d6h   192.168.0.52     master03   <none>           <none>
kube-system   calico-node-n5s7h                          1/1     Running   1 (4h27m ago)   3d6h   192.168.0.62     node03     <none>           <none>
kube-system   calico-node-pjpcw                          1/1     Running   1 (4h27m ago)   3d6h   192.168.0.61     node02     <none>           <none>
kube-system   calico-node-wjbhh                          1/1     Running   1 (4h27m ago)   3d6h   192.168.0.51     master02   <none>           <none>
kube-system   coredns-788958459b-x57st                   1/1     Running   1 (4h27m ago)   3d6h   172.16.196.135   node01     <none>           <none>
kube-system   metrics-server-9989448db-bhf6k             1/1     Running   1 (4h27m ago)   3d5h   172.16.196.134   node01     <none>           <none>


# 查看 STATUS状态和 READY次数是否前后数字一样，确认RESTART数量是否持续增多
```

3.检查集群网段

```bash
[root@master01 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6d16h
[root@master01 ~]# kubectl get po -A -owide
NAMESPACE     NAME                                       READY   STATUS    RESTARTS        AGE    IP               NODE       NOMINATED NODE   READINESS GATES
default       busybox                                    1/1     Running   6 (55m ago)     3d1h   172.16.241.67    master01   <none>           <none>
kube-system   calico-kube-controllers-6d48795585-795b4   1/1     Running   1 (4h36m ago)   3d6h   172.16.196.136   node01     <none>           <none>
kube-system   calico-node-7xxv5                          1/1     Running   1 (4h36m ago)   3d6h   192.168.0.50     master01   <none>           <none>
kube-system   calico-node-b2pwk                          1/1     Running   1 (4h36m ago)   3d6h   192.168.0.60     node01     <none>           <none>
kube-system   calico-node-hp6rt                          1/1     Running   1 (4h36m ago)   3d6h   192.168.0.52     master03   <none>           <none>
kube-system   calico-node-n5s7h                          1/1     Running   1 (4h36m ago)   3d6h   192.168.0.62     node03     <none>           <none>
kube-system   calico-node-pjpcw                          1/1     Running   1 (4h36m ago)   3d6h   192.168.0.61     node02     <none>           <none>
kube-system   calico-node-wjbhh                          1/1     Running   1 (4h36m ago)   3d6h   192.168.0.51     master02   <none>           <none>
kube-system   coredns-788958459b-x57st                   1/1     Running   1 (4h36m ago)   3d6h   172.16.196.135   node01     <none>           <none>
kube-system   metrics-server-9989448db-bhf6k             1/1     Running   1 (4h36m ago)   3d6h   172.16.196.134   node01     <none>           <none>
[root@master01 ~]# kubectl get node -A -owide
NAME       STATUS   ROLES    AGE     VERSION    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME
master01   Ready    <none>   4d15h   v1.28.15   192.168.0.50   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
master02   Ready    <none>   4d6h    v1.28.15   192.168.0.51   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
master03   Ready    <none>   4d6h    v1.28.15   192.168.0.52   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
node01     Ready    <none>   4d6h    v1.28.15   192.168.0.60   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
node02     Ready    <none>   4d6h    v1.28.15   192.168.0.61   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
node03     Ready    <none>   4d6h    v1.28.15   192.168.0.62   <none>        CentOS Linux 7 (Core)   4.19.12-1.el7.elrepo.x86_64   containerd://1.6.33
[root@master01 ~]# kubectl get pod -A -owide |grep coredns
kube-system   coredns-788958459b-x57st                   1/1     Running   1 (4h37m ago)   3d6h   172.16.196.135   node01     <none>           <none>


# 检查得出，我的集群节点网段是 192.168.0.x网段的，我的Pod网段是 172.16.0.0 网段，我的svc是10.96  并没有设置冲突
```

4.能否正常创建资源

```bash
cat<<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
EOF
```

5.Pod 必须能够解析 Service（同 namespace 和跨 namespace）

- `a) nslookup kubernetes`
- `b) nslookup kube-dns.kube-system`

6.每个节点都必须要能访问 Kubernetes 的 kubernetes svc 443 和 kube-dns 的 service 53  

```bash
[root@node03 ~]# curl https://10.96.0.1:443 -k
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {},
  "code": 403
}You have new mail in /var/spool/mail/root
[root@node03 ~]# curl http://10.96.0.10:53 -k
curl: (52) Empty reply from server


# 如果结果和上述一样 就说明正常
```

7.Pod 和 Pod 之间要能够正常通讯（同 namespace 和跨 namespace）

```bash
[root@master01 ~]# kubectl get po -A -o wide
NAMESPACE     NAME                                       READY   STATUS             RESTARTS        AGE    IP               NODE       NOMINATED NODE   READINESS GATES
default       busybox                                    1/1     Running            7 (48m ago)     3d2h   172.16.241.67    master01   <none>           <none>
kube-system   calico-kube-controllers-6d48795585-795b4   1/1     Running            1 (5h30m ago)   3d7h   172.16.196.136   node01     <none>           <none>
kube-system   calico-node-7xxv5                          1/1     Running            1 (5h29m ago)   3d7h   192.168.0.50     master01   <none>           <none>
kube-system   calico-node-b2pwk                          1/1     Running            1 (5h30m ago)   3d7h   192.168.0.60     node01     <none>           <none>
kube-system   calico-node-hp6rt                          1/1     Running            1 (5h29m ago)   3d7h   192.168.0.52     master03   <none>           <none>
kube-system   calico-node-n5s7h                          1/1     Running            1 (5h29m ago)   3d7h   192.168.0.62     node03     <none>           <none>
kube-system   calico-node-pjpcw                          1/1     Running            1 (5h29m ago)   3d7h   192.168.0.61     node02     <none>           <none>
kube-system   calico-node-wjbhh                          1/1     Running            1 (5h29m ago)   3d7h   192.168.0.51     master02   <none>           <none>
kube-system   coredns-788958459b-x57st                   1/1     Running            1 (5h30m ago)   3d7h   172.16.196.135   node01     <none>           <none>
kube-system   metrics-server-9989448db-bhf6k             1/1     Running            1 (5h30m ago)   3d6h   172.16.196.134   node01     <none>           <none>

[root@master01 ~]# kubectl exec -it busybox -- sh
/ # cat /etc/resolv.conf 
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5

/ # ping 172.16.196.136
PING 172.16.196.136 (172.16.196.136): 56 data bytes
64 bytes from 172.16.196.136: seq=0 ttl=62 time=9.009 ms
64 bytes from 172.16.196.136: seq=1 ttl=62 time=1.958 ms
^C
--- 172.16.196.136 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 1.958/5.483/9.009 ms
/ # ping 192.168.0.50
PING 192.168.0.50 (192.168.0.50): 56 data bytes
64 bytes from 192.168.0.50: seq=0 ttl=64 time=0.102 ms
64 bytes from 192.168.0.50: seq=1 ttl=64 time=0.112 ms
^C
--- 192.168.0.50 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.102/0.107/0.112 ms
/ # ping 172.16.196.134 
PING 172.16.196.134 (172.16.196.134): 56 data bytes
64 bytes from 172.16.196.134: seq=0 ttl=62 time=1.719 ms
^C
--- 172.16.196.134 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.719/1.719/1.719 ms

[root@master01 ~]# ping 172.16.241.67
PING 172.16.241.67 (172.16.241.67) 56(84) bytes of data.
64 bytes from 172.16.241.67: icmp_seq=1 ttl=64 time=0.145 ms
```





## FAQ

#### 如何修改 containerd 的默认下载源

```bash
vim /etc/containerd/config.toml
155       [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
# 在 配置文件155行下添加
156         [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
157           endpoint = [
158             "https://zd29wsn0.mirror.aliyuncs.com",
159             "https://docker.m.daocloud.io",
160             "https://dockerproxy.com",
161             "https://mirror.baidubce.com",
162             "https://docker.nju.edu.cn",
163             "https://mirror.iscas.ac.cn"
164           ]

# 重启服务
systemctl restart contained  
```



#### 如何修改默认的上下文配置

1.查看当前集群的上下文和配置

```bash
[root@master01 ~]# kubectl config get-contexts --kubeconfig=/certs/kubeconfig/bootstrap-kubelet.kubeconfig
CURRENT   NAME                                  CLUSTER   AUTHINFO                   NAMESPACE
*         tls-bootstrap-token-user@k8s          k8s       tls-bootstrap-token-user   
          tls-bootstrap-token-user@kubernetes   k8s       tls-bootstrap-token-user   

[root@master01 ~]# kubectl config view --kubeconfig=/certs/kubeconfig/bootstrap-kubelet.kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.0.40:8443
  name: k8s
contexts:
- context:
    cluster: k8s
    user: tls-bootstrap-token-user
  name: tls-bootstrap-token-user@k8s
- context:
    cluster: k8s
    user: tls-bootstrap-token-user
  name: tls-bootstrap-token-user@kubernetes
current-context: tls-bootstrap-token-user@k8s
kind: Config
preferences: {}
users:
- name: tls-bootstrap-token-user
  user:
    token: REDACTED
```

2.删除上下文

```bash
[root@master01 ~]# kubectl config delete-context tls-bootstrap-token-user@kubernetes --kubeconfig=/certs/kubeconfig/bootstrap-kubelet.kubeconfig
deleted context tls-bootstrap-token-user@kubernetes from /certs/kubeconfig/bootstrap-kubelet.kubeconfig
```

3.设置默认上下文

```bash
[root@master01 ~]# kubectl config use-context tls-bootstrap-token-user@k8s --kubeconfig=/certs/kubeconfig/bootstrap-kubelet.kubeconfig
Switched to context "tls-bootstrap-token-user@k8s".

```

























































































































































































