# K8S调度资源-工作负载

## selector声明

`selector` 是 Kubernetes 中用于筛选和匹配特定资源（如 Pod）的机制。它通过标签（labels）对资源进行过滤，以便其他 Kubernetes 对象（如 Deployment、Service、PodDisruptionBudget 等）能够找到并关联到目标资源。 它支持两种方式：`matchLabels` 和 `matchExpressions`。以下是它们的详细说明

### 1. matchLabels

`matchLabels` 是基于标签的简单筛选方式，用于匹配资源的标签键值对。

**参数说明**

- **键值对匹配**：`matchLabels` 是一个字典，键是标签的键名，值是标签的值。
- **逻辑**：`matchLabels` 会筛选出同时满足所有键值对的资源。

```yaml
selector:
  matchLabels:
    app: webserver
    tier: frontend
```

上述配置会匹配所有同时具有 `app: webserver` 和 `tier: frontend` 标签的 Pod。

### 2. matchExpressions

`matchExpressions` 是一种更灵活的筛选方式，支持基于标签的逻辑运算符（如 `In`, `NotIn`, `Exists`, `DoesNotExist` 等）。

**参数说明**

- **operator**：逻辑运算符，用于定义匹配规则。
- **field**：筛选的字段，可以是资源的标签键（默认）或其他字段。
- **values**：用于与 `field` 进行比较的值列表。

**operator 类型**

以下是 `matchExpressions` 支持的主要操作符：

| **Operator**     | **描述**                     | **示例**                                                  |
| ---------------- | ---------------------------- | --------------------------------------------------------- |
| **In**           | 键的值在给定的列表中。       | `app: webserver` 且 `app` 的值在 `[nginx, apache]` 中。   |
| **NotIn**        | 键的值不在给定的列表中。     | `app: webserver` 且 `app` 的值不在 `[nginx, apache]` 中。 |
| **Exists**       | 键存在，但不关心值。         | `app` 标签存在，不管值是什么。                            |
| **DoesNotExist** | 键不存在。                   | `app` 标签不存在。                                        |
| **Gt**           | 键的值大于给定的数值。       | `metrics.cpu` 的值大于 `5`。                              |
| **Lt**           | 键的值小于给定的数值。       | `metrics.cpu` 的值小于 `5`。                              |
| **Gte**          | 键的值大于或等于给定的数值。 | `metrics.cpu` 的值大于等于 `5`。                          |
| **Lte**          | 键的值小于或等于给定的数值。 | `metrics.cpu` 的值小于等于 `5`。                          |

**示例**

```yaml
selector:
  matchExpressions:
    - {key: app, operator: In, values: [nginx, apache]}
    - {key: tier, operator: Exists, values: []}
```

### 3.Selector 的注意事项

1. **唯一性**：
   - 如果多个资源（如 Deployment、Service）使用相同的 `selector`，它们可能会互相干扰。
   - 建议为不同的资源使用不同的标签，避免冲突。
2. **灵活性**：
   - 使用 `matchExpressions` 时，可以实现更复杂的逻辑，但也可能增加配置的复杂性。
   - 根据需求选择合适的匹配方式。
3. **性能**：
   - 过于复杂的 `selector` 可能会影响 Kubernetes 集群的性能。
   - 建议尽量使用简单的匹配规则。

## 1) Replication Controller 和 ReplicaSet

​	在 Kubernetes 中，**Replication Controller** 和 **ReplicaSet** 都是用于确保系统中指定数量的 Pod 副本在任何时刻都能保持运行的控制器。它们有相似的功能，但有一些关键的差异，特别是在 Kubernetes 的版本演进过程中。

### 1.1 Replication Controller

​	Replication Controller 是 Kubernetes 早期的 Pod 副本管理工具，主要用于确保指定数量的 Pod 副本在集群中始终运行。它能够在 Pod 出现故障时创建新的副本，以保证所需的副本数。

**主要功能：**

- 确保 Pod 的副本数始终与期望值一致。
- 如果某些 Pod 停止运行或被删除，Replication Controller 会自动启动新的 Pod 来恢复副本数。

#### 定义一个 Replication Controller 的示例如下：

```yaml
apiVersion: v1  # 指定 API 版本，v1 是稳定版本
kind: ReplicationController  # 资源类型为 ReplicationController
metadata:
  name: nginx  # ReplicationController 的名称
spec:
  replicas: 3  # 设置 Pod 副本数为 3
  selector:  # 标签选择器，用于匹配 Pod
    app: nginx  # 选择标签为 app=nginx 的 Pod
  template:  # Pod 模板，定义将要创建的 Pod 的详细信息
    metadata:
      name: nginx  # Pod 的名称
      labels:  # 标签，用于匹配和选择 Pod
        app: nginx  # 标签 app=nginx
    spec:  # Pod 的规格（包含容器的配置等）
      containers:  # 容器定义
      - name: nginx  # 容器名称
        image: nginx  # 容器使用的镜像
        ports:  # 容器暴露的端口
        - containerPort: 80  # 容器的端口号为 80
```

### 1.2 ReplicaSet

​	ReplicaSet 是 Kubernetes 在 1.2 版本中引入的一个新资源，基本上是 Replication Controller 的升级版。它提供了与 Replication Controller 相同的功能，但支持更多的标签选择器（label selector）。ReplicaSet 引入了 **`Set-based selectors`**（集合选择器），可以更灵活地选择一组 Pod，而不仅仅局限于简单的标签匹配。在实际应用中，ReplicaSet 常常与 Deployment 配合使用来自动管理 Pod 的生命周期。虽然 ReplicaSet 可以单独使用，但通常推荐使用 Deployment，因为 Deployment 提供了更多的功能，如版本回滚、滚动更新等。

**主要功能：**

- 确保 Pod 的副本数始终与期望值一致。
- 提供更强大的标签选择器支持。
- 支持 Pod 的滚动更新与管理，通常与 **Deployment** 控制器配合使用。

### 对比：Replication Controller 与 ReplicaSet

| 特性                    | Replication Controller                               | ReplicaSet                                                   |
| ----------------------- | ---------------------------------------------------- | ------------------------------------------------------------ |
| **定义方式**            | 较老的控制器，主要通过标签选择器选择 Pod。           | 更新版控制器，支持更强大的标签选择器，支持集合选择器。       |
| **选择器**              | 仅支持 **`Equality-based selector`**（等值选择器）。 | 支持 **`Set-based selector`**（集合选择器），提供更多灵活性。 |
| **推荐使用场景**        | 仅用于向后兼容。                                     | 更现代的资源管理，推荐用于新应用。                           |
| **常见用法**            | 适用于简单的副本管理场景。                           | 用于更复杂的管理场景，尤其是与 Deployment 一起使用。         |
| **Deployment 配合使用** | 不直接支持。                                         | 是 Deployment 控制器的核心组成部分。                         |

#### 定义一个 ReplicaSet 的示例如下：

```yaml
apiVersion: apps/v1  # 指定 API 版本，适用于 ReplicaSet
kind: ReplicaSet  # 资源类型为 ReplicaSet
metadata:
  name: frontend  # ReplicaSet 的名称
  labels:
    app: guestbook  # 标签，app=guestbook 用于选择该 ReplicaSet 关联的 Pod
    tier: frontend  # 标签，tier=frontend 用于区分 Pod 的层次
spec:
  replicas: 3  # 设置 Pod 副本数为 3
  selector:  # 标签选择器，用于选择与该 ReplicaSet 相关的 Pod
    matchLabels:  # 基于标签的选择器，匹配标签为 tier=frontend 的 Pod
      tier: frontend
    matchExpressions:  # 更复杂的选择器，支持多个条件
      - {key: tier, operator: In, values: [frontend]}  # 选择标签 key=tier 且值为 frontend 的 Pod
  template:  # Pod 模板，定义新创建 Pod 的规范
    metadata:
      labels:
        app: guestbook  # 标签 app=guestbook，确保与 ReplicaSet 选择器匹配
        tier: frontend  # 标签 tier=frontend，确保与 ReplicaSet 选择器匹配
    spec:  # Pod 的规格
      containers:  # 容器定义
      - name: php-redis  # 容器名称
        image: gcr.io/google_samples/gb-frontend:v3  # 使用的容器镜像
        resources:  # 资源请求
          requests:
            cpu: 100m  # 请求 CPU 资源 100 毫核
            memory: 100Mi  # 请求内存资源 100 MiB
        env:  # 环境变量
        - name: GET_HOSTS_FROM
          value: dns  # 环境变量设置，指定从 DNS 获取主机信息
      ports:
        - containerPort: 80  # 容器开放的端口号
```

#### Deployment 使用 ReplicaSet

当我们使用 **Deployment** 时，Deployment 会自动创建一个 **ReplicaSet** 来管理 Pod 副本。以下是一个 Deployment 的 YAML 文件示例，它会自动创建一个 ReplicaSet 来管理 Pod 副本。

```yaml
apiVersion: apps/v1
kind: Deployment  # 定义为 Deployment 类型
metadata:
  name: nginx-deployment  # Deployment 的名称
spec:
  replicas: 3  # 设置期望的 Pod 副本数为 3
  selector:  # 选择器用于匹配 Pod
    matchLabels:
      app: nginx  # 匹配标签为 app: nginx 的 Pod
  template:  # Pod 模板
    metadata:
      labels:
        app: nginx  # 标签必须与选择器匹配
    spec:
      containers:
        - name: nginx  # 容器名称
          image: nginx:latest  # 使用最新的 nginx 镜像
          ports:
            - containerPort: 80  # 容器暴露端口 80
```

## 2) 无状态应用管理 Deployment

### 2.1 什么是 Deployment？

**Deployment** 是 Kubernetes 中用于管理无状态应用的工作负载的 API。主要用于管理无状态的应用程序，它提供了一种声明式的方式来描述应用的期望状态，确保部署过程中的平滑过渡和回滚。

**主要特点：**

- 适用于无状态服务（Stateless Services）。
- 支持滚动更新（Rolling Update）和回滚（Rollback）。
- 自动管理 ReplicaSet 和 Pod。
- 确保应用在更新过程中保持可用。

### 2.2 Deployment 的 YAML 配置解释

以下是一个典型的 Deployment YAML 配置文件：

```yaml
apiVersion: apps/v1              # 指定使用 apps/v1 API 版本
kind: Deployment                 # 定义这是一个 Deployment
metadata:
  name: nginx-deployment         # Deployment 的名称
  labels:
    app: nginx                   # 为 Deployment 添加标签，方便管理
spec:
  replicas: 3                    # 指定要创建 3 个副本
  selector:                      # 定义选择器，用于筛选管理的 Pod
    matchLabels:                 # 使用基于标签的选择器
      app: nginx                 # 选择所有标签 app=nginx 的 Pod
  template:                      # 定义 Pod 的模板
    metadata:
      labels:
        app: nginx               # 为 Pod 添加标签，确保与 selector 匹配
    spec:
      containers:
      - name: nginx               # 容器名称
        image: m.daocloud.io/docker.io/library/nginx:1.16      # 使用的 Docker 镜像
        ports:
        - containerPort: 80        # 暴露容器的 80 端口
```

### 2.3 Deployment 基础操作

#### 2.3.1 创建 Deployment

使用以下命令创建 Deployment：

```bash
root@k8s-master:~/yaml# kubectl create -f dp-nginx.yaml
deployment.apps/nginx-deployment created
```

**查看 Deployment 状态**

使用 `kubectl get` 和 `kubectl describe` 查看 Deployment 的状态：

```bash
root@k8s-master:~/yaml# kubectl get deploy
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           48m
root@k8s-master:~/yaml# kubectl get po
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-86876b6586-pq2gj   1/1     Running   0          4m46s
nginx-deployment-86876b6586-z2d2q   1/1     Running   0          10m
nginx-deployment-86876b6586-zwqzg   1/1     Running   0          2m8s


#各列说明：
- READY: 		就绪的 Pod 数量 / 总副本数。
- UP-TO-DATE:	已更新到期望状态的副本数。
- AVAILABLE: 	可用的 Pod 数量。
- AGE: 			Deployment 的运行时间。
```

**查看此 Deployment 当前对应的 ReplicaSet：**

```bash
root@k8s-master:~# kubectl get rs -l app=nginx
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-86876b6586   3         3         3       15h

root@k8s-master:~# kubectl get po -l app=nginx
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-86876b6586-76kzl   1/1     Running   0          11h
nginx-deployment-86876b6586-h9qkh   1/1     Running   0          11h
nginx-deployment-86876b6586-rq8w9   1/1     Running   0          11h


#➢ DESIRED：应用程序副本数；
#➢ CURRENT：当前正在运行的副本数；
```

​	当 Deployment 有过更新，对应的 RS 可能不止一个，可以通过-o yaml 获取当前对应的 RS是哪个，其余的 RS 为保留的历史版本，用于回滚等操作。查看此 Deployment 创建的 Pod，可以看到 Pod 的 hash 值 86876b6586和上述 Deployment 对应的 ReplicaSet 的 hash 值一致：

#### 2.3.2 Deployment 更新

> [!CAUTION]
>
> 当且仅当 Deployment 的 Pod 模板（即.spec.template）更改时，才会触发 Deployment
> 更新，例如更改内存、CPU 配置或者容器的 image。

假如更新 Nginx Pod 的 image 使用 m.daocloud.io/docker.io/library/nginx:latest，并使用 --record 记录当前更改的参数，后期回滚时可以查看到对应的信息：

```bash
root@k8s-master:~# kubectl set image deployment/nginx-deployment nginx=m.daocloud.io/docker.io/library/nginx:latest --record
deployment.apps/nginx-deployment image updated
```

**查看更新过后的镜像信息**

```bash
root@k8s-master:~# kubectl get deploy nginx-deployment -o yaml |grep image:
      - image: m.daocloud.io/docker.io/library/nginx:latest
```

**查看资源历史的版本**

```bash
root@k8s-master:~# kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
2         <none>
3         kubectl set image deployment/nginx-deployment nginx=m.daocloud.io/docker.io/library/nginx:latest --record=true
```

**🚨注意**

**<span style="color:red">如果使用set 命令进行资源更新，后续如果想通过原yaml文件进行修改更新那么就需要先将资源导出成为yaml文件然后在进行编辑修改</span>**

**回滚到之前的版本**：

```bash
kubectl rollout undo deployment/nginx-deployment
```

#### 2.3.3 Deployment 扩容

**命令扩容**

```bash
kubectl scale deployment <deployment_name> --replicas=<desired_number_of_replicas>
```

**编辑 Deployment**

通过 `kubectl edit` 命令，直接编辑现有 Deployment 的配置文件，可以修改 `spec.replicas` 数量来实现扩容。编辑后，Kubernetes 会自动更新 Pods 的数量

```bash
kubectl edit deployment <deployment_name>
```

**修改 YAML 文件并应用**

```bash
kubectl apply -f <deployment_yaml_file>
```

#### 2.3.4 暂停和恢复 Deployment 更新

上述演示的均为更改某一处的配置，更改后立即触发更新，大多数情况下可能需要针对一个资源文件更改多处地方，而并不需要多次触发更新，此时可以使用 Deployment 暂停功能，临时禁用更新操作，对 Deployment 进行多次修改后在进行更新。使用 kubectl rollout pause 命令即可暂停 Deployment 更新：

在进行多次配置更改时，可以暂停更新，完成后再恢复：

```bash
kubectl rollout pause deployment/nginx-deployment
```

然后对 Deployment 进行相关更新操作，比如先更新镜像，然后对其资源进行限制（如果使用的是 kubectl edit 命令，可以直接进行多次修改，无需暂停更新，kubectlset 命令一般会集成在CICD 流水线中）：

```bash
# kubectl set image deployment.v1.apps/nginx-deployment nginx=m.daocloud.io/docker.io/library/nginx:1.91
deployment.apps/nginx-deployment image updated

# kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi
deployment.apps/nginx-deployment resource requirements updated
```

通过 rollout history 可以看到没有新的更新：

```bash
root@k8s-master:~# kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
2         <none>
3         kubectl set image deployment/nginx-deployment nginx=m.daocloud.io/docker.io/library/nginx:latest --record=true
```

**进行完最后一处配置更改后，使用 kubectl rollout resume 恢复 Deployment 更新：**

```bash
kubectl rollout resume deployment/nginx-deployment

#恢复状态后可以看到自动把前面做的两次变更进行了更新
root@k8s-master:~# kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
2         <none>
3         kubectl set image deployment/nginx-deployment nginx=m.daocloud.io/docker.io/library/nginx:latest --record=true
4         kubectl set image deployment/nginx-deployment nginx=m.daocloud.io/docker.io/library/nginx:latest --record=true
```

可以看到已经发生了变更
```bash
root@k8s-master:~# kubectl get deploy nginx-deployment -o yaml |grep image:
      - image: m.daocloud.io/docker.io/library/nginx:1.91

root@k8s-master:~# kubectl describe deploy nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Wed, 19 Feb 2025 08:41:33 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 4
                        kubernetes.io/change-cause: kubectl set image deployment/nginx-deployment nginx=m.daocloud.io/docker.io/library/nginx:latest --record=true
Selector:               app=nginx
Replicas:               3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:      m.daocloud.io/docker.io/library/nginx:1.91
    Port:       80/TCP
    Host Port:  0/TCP
    Limits:
      cpu:        200m
      memory:     512Mi
```

### 2.4 更新 Deployment 的注意事项

#### 历史版本清理策略

在默认情况下，Kubernetes 会保留 10 个旧的 ReplicaSet，多余的 ReplicaSet 会在后台被垃圾回收。可以通过 `.spec.revisionHistoryLimit` 设置保留的历史版本数量。如果将其设置为 `0`，则不保留任何历史版本。这对于控制历史版本的存储和清理非常有用，避免集群中积累过多的历史副本。

#### 更新策略

Kubernetes 提供了两种更新策略：

1. **Recreate（重建策略）**
    `.spec.strategy.type == Recreate` 表示在更新时，先删除旧的 Pod，再创建新的 Pod。这种方式适用于需要确保旧 Pod 完全消失后再启动新 Pod 的场景。这种策略适合一些对更新过程中没有 Pod 可用要求较低的场景，通常用于无状态服务或资源充足的环境。

2. **RollingUpdate（滚动更新策略）**
    `.spec.strategy.type == RollingUpdate` 表示逐步替换旧的 Pods，在更新过程中不会停机。滚动更新通过控制更新步长和不可用 Pod 数量来逐步替换旧的 Pod，确保在更新过程中应用始终有可用的 Pod。

   可以通过以下两个参数来控制滚动更新的行为：

   - **`.spec.strategy.rollingUpdate.maxUnavailable`**
      指定在更新过程中最大不可用的 Pod 数量。可以设置为数字或百分比，默认值是 `25%`。例如，如果设置为 `25%`，表示在更新过程中最多有 25% 的 Pod 处于不可用状态。若设置 `maxUnavailable` 为 0，则表示更新过程中不会有任何 Pod 变为不可用。
   - **`.spec.strategy.rollingUpdate.maxSurge`**
      指定在更新过程中，允许超出期望 Pod 数量的最大数量。可以设置为数字或百分比，默认值是 `25%`。如果设置为 `25%`，表示在更新过程中，最多允许 Pod 数量比期望数量多出 25%。这个参数有助于在更新时保持足够的 Pod 来处理流量，避免服务中断。

   > **注意**：如果将 `maxUnavailable` 设置为 `0`，表示在更新过程中不能有 Pod 变为不可用，那么 `maxSurge` 必须大于 0。因为如果没有 `maxSurge`，就无法为更新过程中的不可用 Pod 创建新的 Pod 来替代，从而无法顺利完成更新。

**滚动更新**

```yaml
spec:
  replicas: 3  # 期望的 Pod 副本数
  revisionHistoryLimit: 5  # 保留 5 个历史版本
  strategy:
    type: RollingUpdate  # 使用滚动更新策略
    rollingUpdate:
      maxSurge: 25%  # 允许超出期望数量的 Pod 最大为 25%
      maxUnavailable: 25%  # 更新过程中，最多允许 25% 的 Pod 不可用
      timeoutSeconds: 600  # 滚动更新每次尝试的最大时间，单位秒，默认为 600 秒
      partition: 2  # 只更新特定版本的 Pod，控制从哪些副本开始更新（仅适用于多版本部署场景）
  minReadySeconds: 10  # 新创建的 Pod 在被认为可用之前，必须保持就绪状态的最小时间，单位为秒
  progressDeadlineSeconds: 600  # 更新过程的最长允许时间，单位为秒，若超过此时间更新未完成，则触发回滚
```

 **重建策略 (Recreate)**

```yaml
spec:
  replicas: 3  # 期望的 Pod 副本数
  revisionHistoryLimit: 3  # 保留 3 个历史版本
  strategy:
    type: Recreate  # 使用重建策略
```

#### 为什么 `maxUnavailable` 和 `maxSurge` 不能同时为 0？

如果将 `maxUnavailable` 设置为 0，表示在更新过程中所有 Pod 必须始终保持可用。如果将 `maxSurge` 设置为 0，则意味着在更新过程中不能超出期望 Pod 数量的范围，这将导致没有额外 Pod 用来替换不可用的 Pod，无法保证更新的顺利进行。因此，`maxUnavailable` 和 `maxSurge` 不能同时为 0，避免更新无法进行。

#### 其他相关参数

除了 `maxUnavailable` 和 `maxSurge`，以下参数在滚动更新过程中也起到了重要作用：

- **`spec.minReadySeconds`**
   指定新创建的 Pod 在被认为可用之前，必须保持就绪状态的最小时间（以秒为单位）。默认值为 `0` 秒。这个参数有助于确保新 Pod 完全初始化并准备好接收流量，避免过早接收流量导致服务不稳定。
- **`spec.progressDeadlineSeconds`**
   指定更新过程中允许的最长时间（以秒为单位），如果在此时间内更新进程未达到预期的状态，更新会被视为失败并触发回滚。默认值为 `600` 秒。该参数帮助防止更新过程中发生死锁或卡住的情况。
- **`spec.revisionHistoryLimit`**
   控制历史版本的 ReplicaSet 数量。Kubernetes 会保留最近 `n` 个历史版本，默认为 10，设置为 0 时不保留任何历史版本。该参数帮助管理旧版本的清理。
- **`spec.paused`**
   如果设置为 `true`，Kubernetes 将暂停当前的滚动更新，直到用户手动恢复。这在需要控制更新步伐或避免自动更新的场景中非常有用。

#### 滚动更新行为总结

- **滚动更新的优势**：滚动更新相比于直接重建所有 Pod，能够保证在更新过程中始终有一部分 Pod 可用，从而最小化服务中断的时间。它是高可用性的关键策略之一。
- **更新流程**：Kubernetes 会逐步创建新 Pod，并在新 Pod 启动成功后删除旧 Pod，直至所有 Pod 更新完成。在此过程中，`maxUnavailable` 和 `maxSurge` 共同决定了更新的速度和容忍的可用性损失。

#### 实际应用中的注意事项

- **性能和可用性权衡**：根据应用需求，合理设置 `maxUnavailable` 和 `maxSurge`：
  - 如果要求服务高可用，应该将 `maxUnavailable` 设置为较低的值（如 `0` 或少量），确保在更新过程中保持足够的可用 Pod。
  - 如果可以容忍部分 Pod 短时间不可用，可以适当增加 `maxUnavailable`，同时配置 `maxSurge` 来加快更新过程。
- **资源限制**：如果集群资源有限，设置过高的 `maxSurge` 可能导致资源超载，影响其他服务的稳定性。因此，应该根据集群资源状况和服务需求调整这些参数。



## 3) 无状态应用（Deployment）与有状态应用（StatefulSet）对比

| 特性             | 无状态应用（Deployment）                                     | 有状态应用（StatefulSet）                                    |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **网络身份**     | 无需持久的网络身份，Pod 可以随时创建或删除。Pod 不依赖于固定的 DNS 名称。 | 需要持久的网络身份，Pod 名称唯一且可预测，基于 `{statefulset-name}-{index}` 格式。 |
| **存储**         | 通常不需要持久存储，或者使用共享存储。Pod 的数据可以丢失。   | 需要持久存储，每个 Pod 会绑定一个持久化存储卷（PVC），即使 Pod 被销毁，数据仍然存在。 |
| **扩缩**         | 支持快速扩缩，适合水平扩展。Pod 可以在任意时间创建或删除。   | 扩缩有序，Pod 创建和删除有严格顺序，通常用于集群扩展，保证数据一致性。 |
| **更新策略**     | 支持滚动更新，旧 Pod 删除新 Pod 创建，更新速度较快。         | 有序更新，确保每个 Pod 的状态一致性，Pod 更新时会按照顺序进行，确保依赖性不破坏。 |
| **典型应用**     | Web 服务器、微服务、API 服务等无状态服务。                   | 数据库（如 MySQL、Cassandra）、消息队列（如 Kafka）、分布式存储等需要持久化数据的服务。 |
| **Pod 创建顺序** | Pod 可以无序创建，Kubernetes 会随机选择空闲节点调度 Pod。    | Pod 创建有序，按照顺序创建，从 `0` 开始逐步创建，保证每个 Pod 的状态依赖关系。 |
| **Pod 删除顺序** | Pod 可以无序删除，Kubernetes 会随机选择终止 Pod。            | Pod 删除有序，按照逆序删除，即先删除最后创建的 Pod，后删除最早创建的 Pod。 |
| **自我修复能力** | 支持自我修复，失败 Pod 会自动重启，并会重新创建副本。        | 同样支持自我修复，失败 Pod 会自动重启，但会在固定顺序中恢复。每个 Pod 的名字和存储与集群状态绑定。 |
| **资源管理**     | 通常不需要特别的资源管理。Kubernetes 处理副本和负载均衡。    | 需要持久化资源管理，确保数据不丢失，资源会根据每个 Pod 的状态来配置。 |
| **DNS 解析**     | Pod 名称并不需要保持一致，因此 DNS 地址通常不可预测。        | Pod 名称和 DNS 地址是可预测的，格式为 `{statefulset-name}-{index}.{headless-service-name}`。 |
| **可扩展性**     | 适用于高度可扩展的应用，Pod 可以根据需求快速增加或减少。     | 通常用于扩展有序集群，确保节点和存储之间的一致性和顺序。     |

## 4)  有状态应用管理StatefulSet

### 4.1 什么是StatefulSet？

`StatefulSet` 是 Kubernetes 中用于管理有状态应用的 API 对象。与无状态应用（如使用 `Deployment` 控制器）不同，`StatefulSet` 主要用于那些每个 Pod 都有唯一身份标识且需要稳定存储的应用场景。例如，数据库（如 MySQL、Cassandra）、缓存服务（如 Redis）和分布式队列等。

`StatefulSet` 主要有以下几个特点：

- **稳定的 Pod 名称**：每个 Pod 都会有一个唯一的名字，可以保证在 Pod 重启时，能够根据名字找到对应的存储、网络等资源。
- **持久化存储**：每个 Pod 都有一个持久化的存储卷（PVC），即使 Pod 被删除或重新调度到其他节点，数据依然保持。
- **稳定的网络标识**：Pod 可以使用固定的 DNS 名称来与其他服务进行通信。

### 4.2 Headless Service

**Headless Service** 是 StatefulSet 的关键组成部分，它为每个 Pod 分配一个唯一的 DNS 名称，使得应用程序可以直接访问各个 Pod 而不是负载均衡器的 IP 地址。

**Headless Service 的定义：**

一个 Headless Service 没有 cluster IP 地址，它只提供 DNS 解析服务来实现与各个 Pod 的直接通信。Headless Service 的定义方法是通过设置 `spec.clusterIP: None` 来禁用集群 IP。

示例：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-statefulset-service
spec:
  clusterIP: None  # 关键点，禁用 Cluster IP
  selector:
    app: my-statefulset
  ports:
    - port: 8080
      name: http
```

### 4.3 StatefulSet 的 YAML 配置解释

StatefulSet 的资源文件定义方式与其他 Kubernetes 资源类似，通过 YAML 文件来描述。以下是一个简单的 StatefulSet 示例。

`vim sts-web.yaml`

```yaml
# 定义一个 Headless Service
apiVersion: v1  # 使用 v1 版本的 Kubernetes API
kind: Service  # 服务类型为 Service
metadata:
  name: nginx-headless  # 定义服务的名称为 nginx-headless
  labels:
    app: nginx-headless  # 标签，方便后续匹配
spec:
  ports:
    - port: 80  # 服务暴露的端口为 80
      name: web  # 给这个端口取名为 web
  clusterIP: None  # 设置为 None，表示这是一个 Headless Service（没有 Cluster IP）
  selector:
    app: nginx-sts  # 选择器，选择标签为 app: nginx-sts 的 Pod 来关联服务

---
# 定义 StatefulSet 资源
apiVersion: apps/v1  # 使用 apps/v1 版本的 StatefulSet API
kind: StatefulSet  # 资源类型为 StatefulSet
metadata:
  name: web  # 定义 StatefulSet 的名称为 web
spec:
  serviceName: "nginx-headless"  # 指定使用的 Service 名称为 nginx-headless（Headless Service）
  replicas: 2  # 设置副本数为 2，表示会创建 2 个 Pod
  selector:
    matchLabels:
      app: nginx-sts  # StatefulSet 使用这个 selector 来匹配具有标签 app: nginx-sts 的 Pod
  template:
    metadata:
      labels:
        app: nginx-sts  # 为 Pod 添加标签 app: nginx-sts
    spec:
      containers:
        - name: nginx  # 容器名称为 nginx
          image: m.daocloud.io/docker.io/library/nginx:latest  # 使用 m.daocloud.io/docker.io/library/nginx:latest 镜像
          ports:
            - containerPort: 80  # 容器内的端口为 80
              name: web  # 给容器端口取名为 web
#          volumeMounts:
#            - name: nginx-data  # 挂载名为 nginx-data 的卷
#              mountPath: /usr/share/nginx/html  # 挂载到容器内指定的目录，用于存储静态内容
#  volumeClaimTemplates:
#    - metadata:
#        name: nginx-data  # 持久化存储卷的名字
#      spec:
#        accessModes:
#          - ReadWriteOnce  # 指定访问模式，Pod 只能以读写模式访问卷
#        resources:
#          requests:
#            storage: 1Gi  # 为每个 Pod 请求 1Gi 的存储空间
```

### 4.4 创建 StatefulSet

**创建sts-web**

```bash
root@k8s-master:~/yaml# kubectl apply -f sts-web.yaml
service/nginx-headless created
statefulset.apps/web created
```

**查看sts结果状态**

```bash
root@k8s-master:~/yaml# kubectl get sts
NAME   READY   AGE
web    2/2     18s

#可以看到已经创建并成功运行
```

**查看SVC是否创建成功**

- 服务类型本质上是 `ClusterIP`，但通过设置 `clusterIP: None`，它变成了一个 Headless 服务。
- 这种方式不提供单一的负载均衡 IP，而是让每个 Pod 具有自己的 DNS 名称，你可以直接访问每个 Pod。
- 在 `kubectl get svc` 中查看时，`nginx-headless` 会显示为 `ClusterIP`，但注意它的 `ClusterIP` 被设置为 `None`

通过设置 `ClusterIP: None`，服务不会为其分配一个单一的 IP 地址，而是直接暴露 Pod 的 DNS 名称。Kubernetes 会为每个 Pod 创建一个 DNS 记录，类似于：`web-0.nginx-headless.default.svc.cluster.local`，`web-1.nginx-headless.default.svc.cluster.local`，可以直接通过这些 DNS 名称来访问每个 Pod。

```bash
root@k8s-master:~/yaml# kubectl get svc
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes       ClusterIP   10.96.0.1    <none>        443/TCP   7d21h
nginx-headless   ClusterIP   None         <none>        80/TCP    24s
```

访问headless时需注意，同一命名空间内可以省略命名空间名称，但是不同命名空间尽量避免这种访问模式以规避网络交叉风险

```bash
[root@k8s-master yaml]# kubectl exec -it web-0 -- sh
/ # curl  -I web-1.nginx-headless
HTTP/1.1 200 OK
Server: nginx/1.27.4
Date: Thu, 20 Feb 2025 08:50:09 GMT
Content-Type: text/html
Content-Length: 615
Last-Modified: Wed, 05 Feb 2025 14:46:11 GMT
Connection: keep-alive
ETag: "67a379b3-267"
Accept-Ranges: bytes
```

**根据标签查看sts创建的po**

```bash
root@k8s-master:~/yaml# kubectl get po -l app=nginx-sts
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          88s
web-1   1/1     Running   0          83s
```



### 4.5 StatefulSet 创建 Pod 流程

**StatefulSet 管理的 Pod 部署和扩展规则如下：**

- 对于具有N个副本的StatefulSet，将按顺序从0到N-1开始创建Pod；
- 当删除Pod时，将按照N-1到0的反顺序终止；
- 在缩放Pod之前，必须保证当前的Pod是Running（运行中）或者Ready（就绪）；
- 在终止Pod之前，它所有的继任者必须是完全关闭状态。

> [!CAUTION]
>
> ​	`StatefulSet` 的 `pod.Spec.TerminationGracePeriodSeconds`（终止 Pod 的等待时间）不应该指定  为 0，设置为 0 对 `StatefulSet` 的 Pod 是极其不安全的做法，优雅地删除 StatefulSet 的 Pod 是非常  有必要的，而且是安全的，因为它可以确保在 Kubelet 从 APIServer 删除之前，让 Pod 正常关闭。
> ​	当创建上面的 Nginx 实例时，Pod 将按 web-0、web-1、web-2 的顺序部署 3 个 Pod。在 web0 处于 Running 或者 Ready 之前，web-1 不会被部署，相同的，web-2 在 web-1 未处于 Running  和 Ready 之前也不会被部署。如果在 web-1 处于 Running 和 Ready 状态时，web-0 变成 Failed  （失败）状态，那么 web-2 将不会被启动，直到 web-0 恢复为 Running 和 Ready 状态。
> ​	如果用户将 StatefulSet 的 `replicas` 设置为 1，那么 web-2 将首先被终止，在完全关闭并删除  web-2 之前，不会删除 web-1。如果 web-2 终止并且完全关闭后，web-0 突然失败，那么在 web0 未恢复成 Running 或者 Ready 时，web-1 不会被删除。



### 4.6 StatefulSet 扩容和缩容

和 Deployment 类似，可以通过更新 replicas 字段扩容/缩容 StatefulSet，也可以使用 kubectl scale、kubectl edit 和 kubectl patch 来扩容/缩容一个 StatefulSet。

**kubectl scale扩容命令：**

```bash
[root@k8s-master yaml]# kubectl scale statefulset web --replicas=4
statefulset.apps/web scaled

[root@k8s-master yaml]# kubectl get po -l app=nginx-sts
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          56m
web-1   1/1     Running   0          55m
web-2   1/1     Running   0          51s
web-3   1/1     Running   0          49s

```

**kubectl scale缩容命令：**

扩容和缩容过程中，StatefulSet 会保持 Pod 的顺序编号，并按顺序创建或删除 Pod。

```bash
[root@k8s-master yaml]# kubectl scale statefulset web --replicas=2
statefulset.apps/web scaled

root@k8s-master:~# kubectl get po -l app=nginx-sts -w
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          57m
web-1   1/1     Running   0          56m
web-2   1/1     Running   0          107s
web-3   1/1     Running   0          105s
web-3   1/1     Terminating   0          117s
web-3   1/1     Terminating   0          118s
web-3   0/1     Terminating   0          118s
web-3   0/1     Terminating   0          118s
web-3   0/1     Terminating   0          118s
web-3   0/1     Terminating   0          118s
web-2   1/1     Terminating   0          2m
web-2   1/1     Terminating   0          2m1s
web-2   0/1     Terminating   0          2m1s
web-2   0/1     Terminating   0          2m1s
web-2   0/1     Terminating   0          2m1s
web-2   0/1     Terminating   0          2m1s

[root@k8s-master yaml]# kubectl get po -l app=nginx-sts
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          58m
web-1   1/1     Running   0          57m
```



### 4.7 StatefulSet 更新策略

StatefulSet 拥有两种更新策略，分别为`RollingUpdate`和 `OnDelete`策略

**RollingUpdate策略说明(默认)：**

`RollingUpdate` 会按顺序逐个更新 Pod。你可以通过设置 `partition` 来控制从哪个 Pod 开始更新。`partition` 设置的是 **不会被更新的 Pod** 数量。因此，`partition + 1` 会是开始更新的第一个 Pod。即 `partition` 是已更新 Pod 的数量，所以 **partition 越大，越靠后的 Pod 才会更新**。

**举例说明**：

假设我们有 6 个 Pod，名称分别为 `web-0` 到 `web-5`，如果我们设置 `partition: 3`，那么在更新时，`web-0` 到 `web-2` 不会更新，而从 `web-3` 开始逐个更新。更新顺序是从 `web-3`、`web-4`、`web-5` 开始。

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 3  # 从 web-3 开始更新
```

 **OnDelete 策略说明**

`OnDelete` 策略意味着只有当 Pod 被删除后，StatefulSet 才会创建新的 Pod。因此，你需要手动删除已存在的 Pod，系统才会重新创建并更新它。

**举例说明**：

如果我们有 `web-0` 到 `web-5` 这六个 Pod，当你删除 `web-0` 后，系统会创建新的 Pod 来替代它，直到所有 Pod 都被更新。

```yaml
spec:
  updateStrategy:
    type: OnDelete  # 只有删除后才会创建新的 Pod
```

**RollingUpdate实验验证灰度发布**

```bash
#查看默认的更新策略
[root@k8s-master ~]# kubectl get sts web -o yaml | grep -A 3 "updateStrategy"
  updateStrategy:
    rollingUpdate:
      partition: 0				   # 为0 就是从web-0 开始进行更新
    type: RollingUpdate            #默认更新策略为滚动更新

#修改partition: 为2
kubectl edit sts web
kubectl get sts web -o yaml | grep -A 3 "updateStrategy"
  updateStrategy:
    rollingUpdate:
      partition: 2
    type: RollingUpdate


#查看当前镜像使用的是alpine
[root@k8s-master ~]# kubectl get po -o yaml |grep image:
    - image: m.daocloud.io/docker.io/library/nginx:alpine
      image: m.daocloud.io/docker.io/library/nginx:alpine
    - image: m.daocloud.io/docker.io/library/nginx:alpine
      image: m.daocloud.io/docker.io/library/nginx:alpine
    - image: m.daocloud.io/docker.io/library/nginx:alpine
      image: m.daocloud.io/docker.io/library/nginx:alpine
    - image: m.daocloud.io/docker.io/library/nginx:alpine
      image: m.daocloud.io/docker.io/library/nginx:alpine


#镜像版本更改为 m.daocloud.io/docker.io/library/nginx:1.16并进行验证
kubectl edit sts web

for i in 0 1 2 3; do   echo "web-$i"; kubectl get po web-$i -o jsonpath='{.spec.containers[*].image}';echo ""; done
web-0
m.daocloud.io/docker.io/library/nginx:alpine
web-1
m.daocloud.io/docker.io/library/nginx:alpine
web-2
m.daocloud.io/docker.io/library/nginx:1.16       # 可以看到从 web-2开始进行了更新
web-3
m.daocloud.io/docker.io/library/nginx:1.16

# 通过这种更新方式可以实现分阶段更新，类似于灰度/金丝雀发布。
```

**OnDelete 实验验证**

使用 `OnDelete` 更新策略时，StatefulSet 不会自动更新 Pod。你需要手动删除需要升级的 Pod，系统会自动重新创建并更新它们。以下是实验步骤：

```bash
#检查当前更新策略确保 StatefulSet 的更新策略设置为 OnDelete：
[root@k8s-master ~]# kubectl get sts web -o yaml | grep -A 1 "updateStrategy"
  updateStrategy:
    type: OnDelete				# 确保类型为 OnDelete


#检查当前运行的 Pod 及其使用的镜像版本：
[root@k8s-master ~]# kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].image}{"\n"}{end}'
web-0	m.daocloud.io/docker.io/library/nginx:1.16
web-1	m.daocloud.io/docker.io/library/nginx:1.16
web-2	m.daocloud.io/docker.io/library/nginx:1.16
web-3	m.daocloud.io/docker.io/library/nginx:1.16


#修改镜像版本更新 StatefulSet 使用的镜像版本（例如，从 nginx:1.16更新到 nginx:alpine ）：
kubectl edit sts web

#验证是否有自动更新
[root@k8s-master ~]# kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].image}{"\n"}{end}'
web-0	m.daocloud.io/docker.io/library/nginx:1.16
web-1	m.daocloud.io/docker.io/library/nginx:1.16
web-2	m.daocloud.io/docker.io/library/nginx:1.16
web-3	m.daocloud.io/docker.io/library/nginx:1.16


#选择一个需要更新的 Pod 进行手工删除验证：
kubectl delete pod web-2
这将删除 web-2，随后 StatefulSet 会自动重新创建一个新的 web-2 Pod，使用的是更新后的镜像版本。


#检查重新创建的 Pod 的镜像版本是否已更新：
[root@k8s-master ~]# kubectl get pods 
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          8m26s
web-1   1/1     Running   0          8m29s
web-2   1/1     Running   0          5s
web-3   1/1     Running   0          33m
[root@k8s-master ~]# kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].image}{"\n"}{end}'
web-0	m.daocloud.io/docker.io/library/nginx:1.16
web-1	m.daocloud.io/docker.io/library/nginx:1.16
web-2	m.daocloud.io/docker.io/library/nginx:alpine         #可以看到镜像已经发生了改变
web-3	m.daocloud.io/docker.io/library/nginx:1.16
```

### 4.8 删除 StatefulSet

删除StatefulSet有两种方式，即级联删除和非级联删除。使用非级联方式删除 StatefulSet时，StatefulSet 的 Pod 不会被删除；使用级联删除时，StatefulSet 和它的 Pod 都会被删除。

**非级联删除**

在进行非级联删除时，仅删除 StatefulSet 对象，而保留其关联的 Pod。只需提供--cascade=false 参数即可实现

```bash
kubectl delete sts web --cascade=orphan
```

此命令会删除 `web` StatefulSet，但保留所有关联的 Pod。Pod 将继续运行且受控制器管理。

**级联删除**

在级联删除时，StatefulSet 及其所有关联的 Pod 都会被删除。默认情况下，删除 StatefulSet 使用的是级联删除。命令如下：

```bash
kubectl delete sts web
```

也可以通过创建的yaml文件进行删除

```bash
[root@k8s-master yaml]# kubectl delete -f  sts-web.yaml
service "nginx-headless" deleted
statefulset.apps "web" deleted

#可以看到通过yaml文件创建的svc和sts,po 都已被删除
[root@k8s-master yaml]# kubectl get po,svc，sts
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   8d
```

此命令会删除 `web` StatefulSet 及其所有关联的 Pod，确保集群状态的一致性。

通过以上两种删除策略，你可以根据需求选择是仅删除 StatefulSet 还是同时删除 StatefulSet 与其关联的 Pod。



## 5) 守护进程集 DaemonSet

### 5.1 什么是 DaemonSet？

DaemonSet 是 Kubernetes 中的一种控制器，用于确保在集群中满足匹配规则的的每个（或特定）节点上运行一个或多个相同的 Pod 副本。这些 Pod 通常用于集群范围内的任务，例如：

- **日志收集**：在每个节点上收集日志并将其发送到集中式日志系统。
- **监控代理**：运行监控工具以收集节点和应用的性能指标。
- **网络插件**：确保每个节点都运行网络相关的守护进程。

DaemonSet 与 Deployment 控制器不同，Deployment 专注于跨多个节点运行特定数量的 Pod 副本，而 DaemonSet 确保在每个满足匹配规则（或特定）节点上运行一个 Pod 副本。特定节点的选择可以通过以下条件进行配置：

- **Node Selector**：基于节点标签进行选择。
- **Node Affinity**：基于节点的亲和性规则进行选择，包括强制要求和优先要求。
- **Pod Affinity**：基于 Pod 的亲和性规则进行选择，决定 Pod 是否与其他 Pod 共存于同一节点。

DaemonSet 的主要特点包括：

- **自动部署到新节点**：当集群中有新节点加入时，DaemonSet 会自动在该节点上运行指定的 Pod 副本。
- **支持特定节点**：可以通过节点选择器或节点亲和性指定 DaemonSet 仅在某些节点上运行。
- **独立管理和更新**：可以独立管理和更新集群中的系统服务，而不影响应用层的服务。

### 5.3 DaemonSet的 YAML 配置解释

```yaml
apiVersion: apps/v1        # 必选，指定使用的API版本，表示使用apps/v1版本的DaemonSet
kind: DaemonSet            # 必选，资源类型是DaemonSet，表示这是一个DaemonSet资源
metadata:                  # 必选，资源的元数据部分，包含标签和名称等信息
  labels:                  # 可选，标签，用于标识和选择该DaemonSet
    app: nginx             # 可选，标签键值对，表示这个DaemonSet管理的应用是nginx
  name: nginx              # 必选，DaemonSet的名称，用于唯一标识这个DaemonSet
spec:                      # 必选，DaemonSet的规范部分，定义DaemonSet的期望状态
  selector:                # 必选，选择器，用于选择哪些Pods由该DaemonSet管理
    matchLabels:           # 必选，使用标签匹配来选择相关的Pods
      app: nginx           # 必选，选择标签为app: nginx的Pods
  template:                # 必选，Pod模板，用于创建Pod
    metadata:              # 必选，Pod的元数据部分
      labels:              # 必选，Pod的标签，标识Pod属于nginx应用
        app: nginx         # 必选，Pod的标签，标识Pod属于nginx应用
    spec:                  # 必选，Pod的规格部分，定义Pod内容器的配置
      containers:          # 必选，Pod中的容器列表，DaemonSet至少需要一个容器
        - image: m.daocloud.io/docker.io/library/nginx:alpine  # 必选，容器镜像，指定nginx的版本
          imagePullPolicy: IfNotPresent  # 可选，镜像拉取策略，只有本地没有该镜像时才拉取
          name: nginx             # 必选，容器的名称，指定容器的名称为nginx
```

### 5.4 创建DaemonSet

```bash
[root@k8s-master yaml]# kubectl apply -f ds-nginx.yaml 
daemonset.apps/nginx created

#查看自动创建了两个po，我们并没有指定副本数量
[root@k8s-master yaml]# kubectl get po -owide
NAME          READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES
nginx-45pqp   1/1     Running   0          17s   172.16.84.175    k8s-work2   <none>           <none>
nginx-hkh9w   1/1     Running   0          17s   172.16.182.235   k8s-work1   <none>           <none>

#查看节点是否有设置污点，发现master节点有污点，所以上面没有部署po
[root@k8s-master yaml]# kubectl get nodes -o custom-columns="NODE:.metadata.name,TAINTS:.spec.taints"
NODE         TAINTS
k8s-master   [map[effect:NoSchedule key:node-role.kubernetes.io/control-plane]]
k8s-work1    <none>
k8s-work2    <none>
```

### 5.5 污点设置

**查看污点**

```bash
[root@k8s-master yaml]# kubectl get nodes -o custom-columns="NODE:.metadata.name,TAINTS:.spec.taints"
NODE         TAINTS
k8s-master   [map[effect:NoSchedule key:node-role.kubernetes.io/control-plane]]
k8s-work1    <none>
k8s-work2    <none>

#-o custom-columns="NODE:.metadata.name,TAINTS:.spec.taints"：使用自定义列输出，显示节点的名称和该节点的污点
# k8s-master 节点有污点，阻止普通 Pod 被调度到该节点，通常是为了确保控制平面节点不会被应用程序 Pod 占用。
# k8s-work1 和 k8s-work2 节点没有污点，表示它们可以接收任何 Pod，除非有其他资源限制。
```

**增加污点**

```bash
#命令格式
kubectl taint nodes <node-name> <key>=<value>:<effect>
#	<node-name>: 节点的名称。
#	<key>: 污点的键。
#	<value>: 污点的值。
#	<effect>: 污点的效果，可以是 NoSchedule, PreferNoSchedule, 或 NoExecute


# 键为 forbid，值为 true，并且效果为 NoSchedule。这意味着除非 Pod 容忍这个污点，否则它不会被调度到该节点
kubectl taint nodes k8s-work1 forbid=true:NoSchedule
```

**删除污点**

```bash
kubectl taint nodes k8s-work1 forbid-
```

### 5.6 更新和回滚 DaemonSet

如果添加了新节点或修改了节点标签（Label），DaemonSet 将立刻向新匹配上的节点添加Pod，同时删除不能匹配的节点上的 Pod。
在 Kubernetes 1.6 以后的版本中，可以在 DaemonSet 上执行滚动更新，DaemonSet 更新策略和 StatefulSet 类似，也有 OnDelete 和 RollingUpdate 两种方式。
查看 DaemonSet 更新策略方式：

```bash
[root@k8s-master yaml]# kubectl get ds ds-nginx -o go-template='{{.spec.updateStrategy.type}}{{"\n"}}'
RollingUpdate
```

命令式更新，和之前 Deployment、StatefulSet 方式一致

```bash
kubectl edit ds/<daemonset-name>
kubectl patch ds/<daemonset-name> -p=<strategic-merge-patch>
kubectl set image ds/<daemonset-name><container-name>= <container-newimage> --record=true
```

查看更新状态

```bash
kubectl rollout status ds/<daemonset-name>
```

列出所有修订版本

```bash
kubectl rollout history daemonset <daemonset-name>
```


回滚到指定 revision

```bash
kubectl rollout undo daemonset <daemonset-name> --to-revision=<revision>
```


DaemonSet 的更新和回滚与 Deployment 类似



---

## 6) CronJob 操作文档

### 6.1 创建 CronJob

CronJob 用于在 Kubernetes 中执行定期任务，它允许您按照一定的时间表执行 Jobs。每个 CronJob 定义了一个时间表，并且会周期性地创建 Job 执行任务。

#### 示例：创建一个 CronJob

首先，我们需要创建一个 CronJob 的 YAML 文件，定义执行周期、任务内容等信息。

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: example-cronjob
spec:
  schedule: "*/5 * * * *"  # 每 5 分钟执行一次
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: busybox
              image: busybox
              command:
                - "/bin/sh"
                - "-c"
                - "echo Hello World; sleep 30"
          restartPolicy: OnFailure
```

#### 创建 CronJob

使用 `kubectl create` 命令来创建 CronJob：

```bash
kubectl create -f cronjob-example.yaml
cronjob.batch/example-cronjob created
```

### 6.2 查看 CronJob 状态

#### 查看 CronJob

可以使用 `kubectl get cronjob` 查看 CronJob 的基本信息：

```bash
kubectl get cronjob
NAME               SCHEDULE    SUSPEND   ACTIVE   LAST SCHEDULE   AGE
example-cronjob    */5 * * * * False     0        10s             1m
```

- **SCHEDULE**：CronJob 的时间调度表达式，表示任务的执行频率。
- **SUSPEND**：是否暂停 CronJob，`False` 表示 CronJob 正在运行。
- **ACTIVE**：当前正在运行的 Job 数量。
- **LAST SCHEDULE**：上次执行的时间。
- **AGE**：CronJob 创建的时间。

#### 查看 CronJob 的历史 Jobs

CronJob 会定期创建 Job，您可以通过以下命令查看与 CronJob 相关的历史 Job：

```bash
kubectl get jobs --selector=job-name=example-cronjob
```

#### 查看特定 Job 详情

```bash
kubectl describe job <job-name>
```

### 6.3 更新 CronJob

如果您需要更新 CronJob 的配置（例如更改执行频率或修改容器的镜像），您可以修改 CronJob 的 YAML 文件，然后重新应用：

#### 更新 CronJob

```bash
kubectl apply -f cronjob-example.yaml
```

如果只修改了 CronJob 的时间调度或其他配置，不需要删除 CronJob，而是通过 `kubectl apply` 更新它。

### 6.4 暂停和恢复 CronJob

#### 暂停 CronJob

可以通过以下命令暂停 CronJob，使它不再创建新的 Job：

```bash
kubectl patch cronjob example-cronjob -p '{"spec":{"suspend":true}}'
```

#### 恢复 CronJob

恢复 CronJob 的执行：

```bash
kubectl patch cronjob example-cronjob -p '{"spec":{"suspend":false}}'
```

### 6.5 删除 CronJob

#### 删除 CronJob

可以通过以下命令删除 CronJob，这也会删除与该 CronJob 相关的所有历史 Job：

```bash
kubectl delete cronjob example-cronjob
```

### 6.6 配置 CronJob

#### CronJob 时间表

CronJob 使用标准的 cron 表达式来定义调度计划。以下是常见的 cron 表达式和含义：

| Cron 表达式   | 含义                         |
| ------------- | ---------------------------- |
| `*/5 * * * *` | 每 5 分钟执行一次            |
| `0 * * * *`   | 每小时的第 0 分钟执行一次    |
| `0 0 * * *`   | 每天午夜 12 点执行一次       |
| `0 0 1 * *`   | 每个月的第一天午夜 12 点执行 |
| `0 0 * * 0`   | 每周日午夜 12 点执行一次     |

#### 配置 Job 的重试和失败策略

CronJob 创建的 Job 可以配置失败重试策略：

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: retry-cronjob
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      backoffLimit: 4  # Job 失败后的重试次数，最多重试 4 次
      template:
        spec:
          containers:
            - name: busybox
              image: busybox
              command:
                - "/bin/sh"
                - "-c"
                - "exit 1"  # 强制失败的命令
          restartPolicy: OnFailure
```

- **backoffLimit**：如果 Job 失败，它会重新尝试，最多重试多少次，默认为 6 次。
- **restartPolicy**：指定容器失败后的重启策略，`OnFailure` 表示仅在容器失败时才重启。

#### 配置 Job 的资源限制

您可以为 CronJob 定义资源请求和限制：

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: resource-cronjob
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: busybox
              image: busybox
              command:
                - "/bin/sh"
                - "-c"
                - "echo Hello World"
              resources:
                requests:
                  memory: "64Mi"
                  cpu: "250m"
                limits:
                  memory: "128Mi"
                  cpu: "500m"
          restartPolicy: OnFailure
```

- **resources.requests**：容器需要的最小资源。
- **resources.limits**：容器的最大资源限制。

### 6.7 CronJob 的并发控制

CronJob 在调度时可能会发生并发执行的情况。如果一个 Job 没有在下一个调度周期之前完成，可能会导致多个 Job 同时运行。您可以通过以下策略来控制并发执行行为：

#### 控制并发策略

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cronjob-with-concurrency
spec:
  schedule: "*/5 * * * *"
  concurrencyPolicy: Forbid  # 避免多个 Job 同时执行
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: busybox
              image: busybox
              command:
                - "/bin/sh"
                - "-c"
                - "echo Hello World; sleep 30"
          restartPolicy: OnFailure
```

- **concurrencyPolicy**：
  - **Allow**（默认）：允许 CronJob 任务并发执行。
  - **Forbid**：不允许任务并发执行，如果上一个任务还未完成，下一个任务将被跳过。
  - **Replace**：如果上一个任务还未完成，下一个任务会替换它。

### 6.8 CronJob 配置总结

- **调度频率**：通过 `schedule` 属性定义 cron 表达式，指定 CronJob 的执行频率。
- **重试策略**：通过 `backoffLimit` 控制任务失败后的重试次数。
- **资源限制**：通过 `resources.requests` 和 `resources.limits` 配置资源请求和限制。
- **并发控制**：使用 `concurrencyPolicy` 控制任务是否允许并发执行。

---

# 检查资源调度

### **检查集群中资源的调度规则是否为单节点**

```bash
while read workLoad resourceName nameSpaces; do
    echo "工作负载类型：$workLoad 资源名称：$resourceName 命名空间：$nameSpaces"
    kubectl get $workLoad $resourceName -n "$nameSpaces" -o yaml |
    awk '/^\s*affinity:/, /containers:/ { if ($0 ~ /^\s*affinity:/ || $0 !~ /^\s*containers:/) print }'
done < list
```

### **检查是否在k8s中部署数据库：**

```bash
while read workLoad resourceName nameSpaces; do
    echo "工作负载类型：$workLoad 资源名称：$resourceName 命名空间：$nameSpaces"
    kubectl get $workLoad $resourceName -n "$nameSpaces" -o yaml | grep image: | egrep -i "mysql|PostgreSQL|Oracle|SQLite|Redis|Memcached|MongoDB" && \
    (echo "工作负载类型：$workLoad 资源名称：$resourceName 命名空间：$nameSpaces" >> result && kubectl get $workLoad $resourceName -n "$nameSpaces" -o yaml | grep image: | egrep -i "mysql|PostgreSQL|Oracle|SQLite|Redis|Memcached|MongoDB" >> result) || echo "没有匹配的镜像"
done < list
```



